{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automation Homework\n",
    "\n",
    "#### Goal: Identify and download annual reports from http://www.annualreports.com\n",
    "\n",
    "##### Author: Alex Sherman | alsherman@deloitte.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from configparser import ConfigParser, ExtendedInterpolation\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.core.display import display, HTML\n",
    "from urllib import robotparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Use configparser to get the following variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = ConfigParser(interpolation=ExtendedInterpolation())\n",
    "config.read('../../config.ini')\n",
    "\n",
    "OUTPUT_DIR_PATH =\n",
    "BASE_URL =\n",
    "COMPANY ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a string with the url to the webpage with the selected companies annual reports\n",
    "# use python string formatting .format() to combine the base_url with the company\n",
    "\n",
    "company_url = r'{}/Company/{}'.format(BASE_URL, COMPANY)\n",
    "company_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Check the robots.txt to confirm access\n",
    "\n",
    "Before writing any code check the http://www.annualreports.com/robots.txt to ascertain any data collection restrictions. \n",
    "\n",
    "User-agent: * applies to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=http://www.annualreports.com/robots.txt  width=500 height=250></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests_url = 'http://www.annualreports.com/robots.txt '\n",
    "iframe = '<iframe src={} width=500 height=250></iframe>'.format(requests_url)\n",
    "HTML(iframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'company_url' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-36622c5a807e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"http://www.annualreports.com/robots.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mrp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mrp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcan_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"*\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompany_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'company_url' is not defined"
     ]
    }
   ],
   "source": [
    "# confirm that we are allowed to access the data\n",
    "rp = robotparser.RobotFileParser()\n",
    "rp.set_url(\"http://www.annualreports.com/robots.txt\")\n",
    "rp.read()\n",
    "rp.can_fetch(\"*\", company_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Collect all of the urls for the numerous annual reports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# view the source data website\n",
    "iframe = '<iframe src={} width=950 height=300></iframe>'.format(company_url)\n",
    "HTML(iframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# request the html from the company_url\n",
    "r = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert the text from the request into a BeautifulSoup instance\n",
    "b = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find all of the links to the pdfs - <a href=\"example_url.pdf\">\n",
    "# look for any tags that contains all of the links\n",
    "# just collect the html, we will extract the links in the next exercise\n",
    "\n",
    "annual_reports = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create an empty list to store the urls\n",
    "urls = \n",
    "\n",
    "# iterate through the annual_reports\n",
    "\n",
    "    # find the report url ending\n",
    "    report_name = \n",
    "\n",
    "    # combine the base_url with the report name to create the full url\n",
    "    # consider using the string method: join\n",
    "    report_url = \n",
    "\n",
    "    # append each report_url to the urls list \n",
    "    \n",
    "    \n",
    "# view the first 5 results\n",
    "urls[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a mapping a file paths with the filename and filepath (to store each file locally)\n",
    "Store all files in a folder on your desktop called ml_guild/raw_data/[company_name]\n",
    "- NOTE: replace [company_name] with the selected companies name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create an empty dict to store file paths\n",
    "# name is output_paths\n",
    "output_paths = \n",
    "\n",
    "# iterate through the urls\n",
    "# consider using enumerate to get the index of the url in the list\n",
    "\n",
    "    # parse the year from the annual report report_name\n",
    "    # split and slice the url to extract the year\n",
    "    year = \n",
    "\n",
    "    # The first annual report on a page is stored in different html\n",
    "    # and does not have the year in the report name\n",
    "    # e.g. ('Click/[#]') instead of ('NYSE_ORCL_2015.pdf')\n",
    "    # add a condition to identify the url with index 0 and\n",
    "    # add one to the year of the next annual report (2nd most recent year)\n",
    "    # you will need to convert a string to an int for the addition\n",
    "    \n",
    "    \n",
    "    # create a file name \n",
    "    # use the naming scheme companyname_annual_report_year\n",
    "    # use .format() to replace the companyname and year for each report\n",
    "    filename =\n",
    "    \n",
    "    # create a local filepath to identify how to name a file\n",
    "    # and where to store it locally\n",
    "    filepath = \n",
    "    \n",
    "    # add each url to the output_paths dict\n",
    "\n",
    "\n",
    "# view results\n",
    "output_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Download all of the annual reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# iterate through the urls\n",
    "\n",
    "\n",
    "    # get the path of where to download the pdf locally\n",
    "    # use the url to get the filepath from the output_paths dict\n",
    "    filepath =\n",
    "    \n",
    "    # download the pdf with requests\n",
    "    r =\n",
    "    \n",
    "    # write the pdf to the filepath\n",
    "    # 'wb' stands for write binary\n",
    "    with open(filepath, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    \n",
    "    # required delay, stated in the robots.txt\n",
    "    time.sleep(5)  # pause for five seconds\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
