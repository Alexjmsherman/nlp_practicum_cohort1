{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines, RandomizedSearch, and Transformers\n",
    "\n",
    "##### Author: Alex Sherman | alsherman@deloitte.com\n",
    "\n",
    "##### Agenda:\n",
    "1. Pipelines\n",
    "2. Advanced Cross Validation (randomized Hyperparameter tuning)\n",
    "3. Feature Union (combining pipelines)\n",
    "4. Custom Transformers (customizing pipeline components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import defaultdict\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core.display import display, HTML\n",
    "from configparser import ConfigParser, ExtendedInterpolation\n",
    "import spacy\n",
    "from sqlalchemy import create_engine\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import  RandomizedSearchCV\n",
    "from sklearn.preprocessing import Imputer, StandardScaler, OneHotEncoder, LabelBinarizer, FunctionTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, SelectPercentile\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn import datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration for data, acronyms, and gensim paths\n",
    "config = ConfigParser(interpolation=ExtendedInterpolation())\n",
    "config.read('../../config.ini')\n",
    "\n",
    "DB_PATH = config['DATABASES']['PROJECT_DB_PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>section_name</th>\n",
       "      <th>section_text</th>\n",
       "      <th>criteria</th>\n",
       "      <th>section_length</th>\n",
       "      <th>contains_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>southwest-airlines-co_annual_report_1994.docx</td>\n",
       "      <td>FIRST SECTION</td>\n",
       "      <td>© 1994 Southwest Airlines Co. This annual repo...</td>\n",
       "      <td>&lt;function heading at 0x000001D4AA492EA0&gt;</td>\n",
       "      <td>83</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>southwest-airlines-co_annual_report_1994.docx</td>\n",
       "      <td>TABLE OF CONTENTS CONSOLIDATED HIGHLIGHTS</td>\n",
       "      <td>(Dollars in thousands except per share amounts...</td>\n",
       "      <td>&lt;function heading at 0x000001D4AA492EA0&gt;</td>\n",
       "      <td>113</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>southwest-airlines-co_annual_report_1994.docx</td>\n",
       "      <td>NET INCOME NET INCOME PER SHARE LOW FARES</td>\n",
       "      <td>Southwest Airlines was built, from the ground ...</td>\n",
       "      <td>&lt;function heading at 0x000001D4AA492EA0&gt;</td>\n",
       "      <td>1553</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>southwest-airlines-co_annual_report_1994.docx</td>\n",
       "      <td>TO OUR SHAREHOLDERS</td>\n",
       "      <td>In 1994, Southwest Airlines produced a profit ...</td>\n",
       "      <td>&lt;function heading at 0x000001D4AA492EA0&gt;</td>\n",
       "      <td>4862</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>southwest-airlines-co_annual_report_1994.docx</td>\n",
       "      <td>LOW COST</td>\n",
       "      <td>Southwest has the lowest cost structure among ...</td>\n",
       "      <td>&lt;function heading at 0x000001D4AA492EA0&gt;</td>\n",
       "      <td>2393</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   section_id                                       filename  \\\n",
       "0           1  southwest-airlines-co_annual_report_1994.docx   \n",
       "1           2  southwest-airlines-co_annual_report_1994.docx   \n",
       "2           3  southwest-airlines-co_annual_report_1994.docx   \n",
       "3           4  southwest-airlines-co_annual_report_1994.docx   \n",
       "4           5  southwest-airlines-co_annual_report_1994.docx   \n",
       "\n",
       "                                section_name  \\\n",
       "0                              FIRST SECTION   \n",
       "1  TABLE OF CONTENTS CONSOLIDATED HIGHLIGHTS   \n",
       "2  NET INCOME NET INCOME PER SHARE LOW FARES   \n",
       "3                        TO OUR SHAREHOLDERS   \n",
       "4                                   LOW COST   \n",
       "\n",
       "                                        section_text  \\\n",
       "0  © 1994 Southwest Airlines Co. This annual repo...   \n",
       "1  (Dollars in thousands except per share amounts...   \n",
       "2  Southwest Airlines was built, from the ground ...   \n",
       "3  In 1994, Southwest Airlines produced a profit ...   \n",
       "4  Southwest has the lowest cost structure among ...   \n",
       "\n",
       "                                   criteria  section_length  contains_fee  \n",
       "0  <function heading at 0x000001D4AA492EA0>              83         False  \n",
       "1  <function heading at 0x000001D4AA492EA0>             113         False  \n",
       "2  <function heading at 0x000001D4AA492EA0>            1553         False  \n",
       "3  <function heading at 0x000001D4AA492EA0>            4862         False  \n",
       "4  <function heading at 0x000001D4AA492EA0>            2393         False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine = create_engine(DB_PATH)\n",
    "df = pd.read_sql(\"SELECT * FROM Sections\", con=engine)\n",
    "\n",
    "# filter to relevant sections\n",
    "df['contains_fee'] = df['section_text'].str.contains('fee')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2810, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the row count\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the section names to determine if the section text will contain the term fee\n",
    "X = df['section_name']\n",
    "y = df['contains_fee']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example ML Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.95      0.98      0.97       655\n",
      "       True       0.56      0.31      0.40        48\n",
      "\n",
      "avg / total       0.92      0.94      0.93       703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set up ML experiment\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# tfidf transform data\n",
    "tfidf = TfidfVectorizer()\n",
    "fit_vect = tfidf.fit_transform(X_train)\n",
    "\n",
    "# lsi transform data\n",
    "lsi = TruncatedSVD(random_state=42)\n",
    "fit_lsi = lsi.fit_transform(fit_vect)\n",
    "\n",
    "# build random forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(fit_lsi, y_train)\n",
    "\n",
    "# make predictions\n",
    "test_vect = tfidf.transform(X_test)\n",
    "test_lsi = lsi.transform(test_vect)\n",
    "y_pred = rf.predict(test_lsi)\n",
    "\n",
    "# evaluate prediction\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example INCORRECT(!!!) cross validation\n",
    "\n",
    "In the below process, we transform the data with tfidf and lsi before passing it to the RandomizedSearchCV\n",
    "\n",
    "Problems:\n",
    "1. All the data is passed to tfidf, so it learns the full vocabulary, instead of the vocabulary from only the training data\n",
    "2. All the data is passed to lsi, so it learns the full data, instead of the from only the training data\n",
    "3. We are only tuning the model hyperparameters instead of all transformer and feature selection hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.610525</td>\n",
       "      <td>0.043808</td>\n",
       "      <td>0.360432</td>\n",
       "      <td>0.830723</td>\n",
       "      <td>285</td>\n",
       "      <td>{'n_estimators': 285}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.096386</td>\n",
       "      <td>0.881057</td>\n",
       "      <td>0.568807</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.796296</td>\n",
       "      <td>0.006577</td>\n",
       "      <td>0.007317</td>\n",
       "      <td>0.196951</td>\n",
       "      <td>0.036386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.123836</td>\n",
       "      <td>0.115166</td>\n",
       "      <td>0.360049</td>\n",
       "      <td>0.830723</td>\n",
       "      <td>435</td>\n",
       "      <td>{'n_estimators': 435}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.881057</td>\n",
       "      <td>0.568807</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.796296</td>\n",
       "      <td>0.084316</td>\n",
       "      <td>0.046530</td>\n",
       "      <td>0.197465</td>\n",
       "      <td>0.036386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.795178</td>\n",
       "      <td>0.061156</td>\n",
       "      <td>0.359102</td>\n",
       "      <td>0.830723</td>\n",
       "      <td>350</td>\n",
       "      <td>{'n_estimators': 350}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.881057</td>\n",
       "      <td>0.563636</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.796296</td>\n",
       "      <td>0.107860</td>\n",
       "      <td>0.006699</td>\n",
       "      <td>0.194608</td>\n",
       "      <td>0.036386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.044502</td>\n",
       "      <td>0.005002</td>\n",
       "      <td>0.355562</td>\n",
       "      <td>0.821888</td>\n",
       "      <td>20</td>\n",
       "      <td>{'n_estimators': 20}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.878261</td>\n",
       "      <td>0.563636</td>\n",
       "      <td>0.798165</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.789238</td>\n",
       "      <td>0.002830</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.199371</td>\n",
       "      <td>0.040028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.442824</td>\n",
       "      <td>0.023832</td>\n",
       "      <td>0.354600</td>\n",
       "      <td>0.834162</td>\n",
       "      <td>140</td>\n",
       "      <td>{'n_estimators': 140}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.094118</td>\n",
       "      <td>0.881057</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.098068</td>\n",
       "      <td>0.005720</td>\n",
       "      <td>0.192664</td>\n",
       "      <td>0.034295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "3       0.610525         0.043808         0.360432          0.830723   \n",
       "1       1.123836         0.115166         0.360049          0.830723   \n",
       "4       0.795178         0.061156         0.359102          0.830723   \n",
       "2       0.044502         0.005002         0.355562          0.821888   \n",
       "0       0.442824         0.023832         0.354600          0.834162   \n",
       "\n",
       "  param_n_estimators                 params  rank_test_score  \\\n",
       "3                285  {'n_estimators': 285}                1   \n",
       "1                435  {'n_estimators': 435}                2   \n",
       "4                350  {'n_estimators': 350}                3   \n",
       "2                 20   {'n_estimators': 20}                4   \n",
       "0                140  {'n_estimators': 140}                5   \n",
       "\n",
       "   split0_test_score  split0_train_score  split1_test_score  \\\n",
       "3           0.096386            0.881057           0.568807   \n",
       "1           0.095238            0.881057           0.568807   \n",
       "4           0.097561            0.881057           0.563636   \n",
       "2           0.086957            0.878261           0.563636   \n",
       "0           0.094118            0.881057           0.553571   \n",
       "\n",
       "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "3            0.814815           0.416667            0.796296      0.006577   \n",
       "1            0.814815           0.416667            0.796296      0.084316   \n",
       "4            0.814815           0.416667            0.796296      0.107860   \n",
       "2            0.798165           0.416667            0.789238      0.002830   \n",
       "0            0.821429           0.416667            0.800000      0.098068   \n",
       "\n",
       "   std_score_time  std_test_score  std_train_score  \n",
       "3        0.007317        0.196951         0.036386  \n",
       "1        0.046530        0.197465         0.036386  \n",
       "4        0.006699        0.194608         0.036386  \n",
       "2        0.000409        0.199371         0.040028  \n",
       "0        0.005720        0.192664         0.034295  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# tfidf transform data\n",
    "tfidf = TfidfVectorizer()\n",
    "fit_vect = tfidf.fit_transform(X)  # wrong! - using all data \n",
    "\n",
    "# lsi transform data\n",
    "lsi = TruncatedSVD(random_state=42)\n",
    "fit_lsi = lsi.fit_transform(fit_vect)  # wrong! - using all data\n",
    "\n",
    "# hyperparameters to test\n",
    "param_dist = {'n_estimators':range(10,500,5)}  # limited to model hyperparameters\n",
    "\n",
    "# set cross validatation to test n random hyperparameters\n",
    "grid = RandomizedSearchCV(\n",
    "      RandomForestClassifier(random_state=42)\n",
    "    , param_distributions=param_dist\n",
    "    , n_iter=5\n",
    "    , cv=3\n",
    "    , refit='f1'\n",
    "    , scoring='f1'\n",
    "    , return_train_score=True\n",
    ")\n",
    "grid.fit(fit_lsi,y)\n",
    "\n",
    "# review results\n",
    "pd.DataFrame(grid.cv_results_).sort_values('mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "Pipeline of transforms with a final estimator.\n",
    "\n",
    "Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The final estimator only needs to implement fit. The transformers in the pipeline can be cached using memory argument.\n",
    "\n",
    "The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a double underscore ‘__’. A step’s estimator may be replaced entirely by setting the parameter with its name to another estimator, or a transformer removed by setting to None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines in sklearn\n",
    "\n",
    "\n",
    "Pipelines chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification. The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. \n",
    "\n",
    "Pipelines Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The final estimator only needs to implement fit. The transformers in the pipeline can be cached using memory argument.\n",
    "\n",
    "Pipeline serves serveral purposes:\n",
    "\n",
    "- **Convenience and encapsulation**: You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n",
    "\n",
    "- **Joint parameter selection**: You can grid search over parameters of all estimators in the pipeline at once.\n",
    "\n",
    "- **Safety**: Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors.\n",
    "All estimators in a pipeline, except the last one, must be transformers (i.e. must have a transform method). The last estimator may be any type (transformer, classifier, etc.)\n",
    "\n",
    "SOURCE:\n",
    "- [Pipeline: chaining estimators](http://scikit-learn.org/stable/modules/pipeline.html#pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# define a pipeline\n",
    "# the pipeline includes a list of steps to complete\n",
    "# each step is a tuple with a name for the step and an uninstantiated class \n",
    "pipe = Pipeline([\n",
    "      ('tfidf', TfidfVectorizer(stop_words='english'))\n",
    "    , ('lsi', TruncatedSVD(random_state=42))\n",
    "    , ('clf', LogisticRegression(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "# fitting a pipeline is the same as calling fit on every intermediate steps\n",
    "pipe.fit(X_train,y_train)\n",
    "\n",
    "# predict using the model at the end of the pipeline\n",
    "y_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.95      0.29      0.45       655\n",
      "       True       0.07      0.77      0.13        48\n",
      "\n",
      "avg / total       0.89      0.32      0.42       703\n",
      "\n",
      "accuracy: 0.32432432432432434\n"
     ]
    }
   ],
   "source": [
    "# evaluate prediction\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('accuracy: {}'.format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### view the results of the model on the testing datast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Type: FALSE POSITIVES\n",
      "                                           section_name  contains_fee  pred\n",
      "2347               OFFSETTING OF DERIVATIVE LIABILITIES         False  True\n",
      "1097  EMPLOYEE GROUP REPRESENTED BY AGREEMENT AMENDA...         False  True\n",
      "2271                                 OPERATING EXPENSES         False  True\n",
      "565                                   DEBORAH  ACKERMAN         False  True\n",
      "1543  OPERATING EXPENSES PER ASM, NON-GAAP, EXCLUDIN...         False  True \n",
      "\n",
      "\n",
      "Result Type: FALSE NEGATIVES\n",
      "                                           section_name  contains_fee   pred\n",
      "1260  THE COMPANY’S LOW COST STRUCTURE IS ONE OF ITS...          True  False\n",
      "2652  THE COMPANY’S LOW-COST STRUCTURE HAS HISTORICA...          True  False\n",
      "233   TO OUR SHAREHOLDERS: 1997 WAS SOUTHWEST AIRLIN...          True  False\n",
      "457   LAST YEAR, IN THE WAKE OF THE NATIONAL TRAGEDY...          True  False\n",
      "1739  THE COMPANY’S LOW-COST STRUCTURE HAS HISTORICA...          True  False \n",
      "\n",
      "\n",
      "Result Type: TRUE POSITIVES\n",
      "                                           section_name  contains_fee  pred\n",
      "1539  ESTIMATED DIFFERENCE IN ECONOMIC JET FUEL PRIC...          True  True\n",
      "1700                                            AIRTRAN          True  True\n",
      "2223                         PRICING AND COST STRUCTURE          True  True\n",
      "1283  AIRTRAN IS CURRENTLY SUBJECT TO PENDING ANTITR...          True  True\n",
      "1357                                             LEASES          True  True \n",
      "\n",
      "\n",
      "Result Type: TRUE NEGATIVES\n",
      "                                           section_name  contains_fee   pred\n",
      "2373  PART III ITEM 10. DIRECTORS, EXECUTIVE OFFICER...         False  False\n",
      "1487  ITEM 1A. RISK FACTORS THE COMPANY’S BUSINESS H...         False  False\n",
      "2230   DISCLOSURE REGARDING FORWARD-LOOKING INFORMATION         False  False\n",
      "1690  SECURITIES REGISTERED PURSUANT TO SECTION 12(G...         False  False\n",
      "1099   DISCLOSURE REGARDING FORWARD-LOOKING INFORMATION         False  False \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# optional - expand the column width to see more text\n",
    "pd.set_option('max_colwidth',50)\n",
    "\n",
    "# combine X_test/y_test and add the prediction as a new column\n",
    "# to review model predictions\n",
    "df_test = pd.concat([X_test, y_test], axis=1) \n",
    "df_test['pred'] = y_pred  \n",
    "\n",
    "# define the quadrants of a confusion matrix\n",
    "fp = (df_test.contains_fee == False) & (df_test.pred == True)\n",
    "fn = (df_test.contains_fee == True) & (df_test.pred == False)\n",
    "tp = (df_test.contains_fee == True) & (df_test.pred == True)\n",
    "tn = (df_test.contains_fee == False) & (df_test.pred == False)\n",
    "\n",
    "confusion_matrix = [\n",
    "      ('FALSE POSITIVES',fp)\n",
    "    , ('FALSE NEGATIVES',fn)\n",
    "    , ('TRUE POSITIVES',tp)\n",
    "    , ('TRUE NEGATIVES',tn)\n",
    "]\n",
    "\n",
    "# view examples of each type of result in the confusion matrix\n",
    "# use this to help determine new useful feautes to improve the model\n",
    "for key, val in confusion_matrix:\n",
    "    print('Result Type: {}'.format(key))\n",
    "    print(df_test[val][['section_name','contains_fee','pred']][0:5], '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...alty='l2', random_state=42,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the entire pipeline\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf': LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "           multi_class='ovr', n_jobs=1, penalty='l2', random_state=42,\n",
       "           solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       " 'lsi': TruncatedSVD(algorithm='randomized', n_components=2, n_iter=5,\n",
       "        random_state=42, tol=0.0),\n",
       " 'tfidf': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None)}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view a dict of each step in the pipeline\n",
    "# this is useful to select a single fit transformer or estimator\n",
    "pipe.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select only the tfidfvectorizer\n",
    "pipe.named_steps['tfidf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_char_ngrams', '_char_wb_ngrams', '_check_vocabulary', '_count_vocab', '_get_param_names', '_limit_features', '_sort_features', '_tfidf', '_validate_vocabulary', '_white_spaces', '_word_ngrams', 'analyzer', 'binary', 'build_analyzer', 'build_preprocessor', 'build_tokenizer', 'decode', 'decode_error', 'dtype', 'encoding', 'fit', 'fit_transform', 'fixed_vocabulary_', 'get_feature_names', 'get_params', 'get_stop_words', 'idf_', 'input', 'inverse_transform', 'lowercase', 'max_df', 'max_features', 'min_df', 'ngram_range', 'norm', 'preprocessor', 'set_params', 'smooth_idf', 'stop_words', 'stop_words_', 'strip_accents', 'sublinear_tf', 'token_pattern', 'tokenizer', 'transform', 'use_idf', 'vocabulary', 'vocabulary_']\n"
     ]
    }
   ],
   "source": [
    "print(dir(pipe.named_steps['tfidf']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['agreements', 'ahead', 'ai', 'air', 'aircraft', 'airframe', 'airframes', 'airline', 'airlines', 'airplane', 'airplanes', 'airport', 'airtran', 'aisle', 'akron', 'alan', 'alaska', 'alk', 'allegiance', 'allow', 'amadeus', 'amendable', 'amendment', 'amenities', 'america', 'americans', 'amex', 'amfa', 'amortization', 'amounts', 'amr', 'analysis', 'analysts', 'ancillary', 'angeles', 'announced', 'annual', 'annualized', 'answer', 'anticipated', 'antitrust', 'aoci', 'apbo', 'app', 'application', 'applications', 'approach', 'approval', 'approximate', 'approximately', 'april', 'arca', 'area', 'arrangements', 'arrivals', 'aruba', 'asm', 'asset', 'assets', 'associated', 'assumed', 'atlanta', 'attacks', 'attorney', 'attributable', 'auditors', 'august', 'austin', 'authorities', 'authorized', 'availability', 'available', 'average', 'aviation', 'awa', 'award', 'awards', 'bad', 'baggage', 'bahamas', 'balance', 'bargaining', 'barrett', 'barron', 'barry', 'barshop', 'based', 'basic', 'basis', 'bates', 'began', 'beginning', 'beginnings', 'believe', 'benchmark', 'beneficial', 'benefit', 'benefits', 'better', 'beverages']\n"
     ]
    }
   ],
   "source": [
    "# extract the features names from the fit tfidf in the pipeline\n",
    "feature_names = pipe.named_steps['tfidf'].get_feature_names()\n",
    "\n",
    "# print some features names learned by tfidf\n",
    "# ignore the first few hundred which are primarily numbers\n",
    "print(feature_names[300:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_get_param_names', 'algorithm', 'components_', 'explained_variance_', 'explained_variance_ratio_', 'fit', 'fit_transform', 'get_params', 'inverse_transform', 'n_components', 'n_iter', 'random_state', 'set_params', 'singular_values_', 'tol', 'transform']\n"
     ]
    }
   ],
   "source": [
    "print(dir(pipe.named_steps['lsi']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the variance ratio from the truncatedSVD (latent semantic indexing)\n",
    "pipe.named_steps['lsi'].n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_estimator_type', '_get_param_names', '_predict_proba_lr', 'class_weight', 'classes_', 'coef_', 'decision_function', 'densify', 'dual', 'fit', 'fit_intercept', 'get_params', 'intercept_', 'intercept_scaling', 'max_iter', 'multi_class', 'n_iter_', 'n_jobs', 'penalty', 'predict', 'predict_log_proba', 'predict_proba', 'random_state', 'score', 'set_params', 'solver', 'sparsify', 'tol', 'verbose', 'warm_start']\n"
     ]
    }
   ],
   "source": [
    "print(dir(pipe.named_steps['clf']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.84457029, -1.28076678]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps['clf'].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation with pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. Typical examples include C, kernel and gamma for Support Vector Classifier, alpha for Lasso, etc.\n",
    "\n",
    "It is possible and recommended to search the hyper-parameter space for the best cross validation score.\n",
    "\n",
    "Two generic approaches to sampling search candidates are provided in scikit-learn: for given values, GridSearchCV exhaustively considers all parameter combinations, while RandomizedSearchCV can sample a given number of candidates from a parameter space with a specified distribution. After describing these tools we detail best practice applicable to both approaches.\n",
    "\n",
    "Note that it is common that a small subset of those parameters can have a large impact on the predictive or computation performance of the model while others can be left to their default values. It is recommended to read the docstring of the estimator class to get a finer understanding of their expected behavior, possibly by reading the enclosed reference to the literature.\n",
    "\n",
    "\n",
    "##### RandomizedSearchCV\n",
    "While using a grid of parameter settings is currently the most widely used method for parameter optimization, other search methods have more favourable properties. RandomizedSearchCV implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search:\n",
    "\n",
    "A budget can be chosen independent of the number of parameters and possible values.\n",
    "Adding parameters that do not influence the performance does not decrease efficiency.\n",
    "Specifying how parameters should be sampled is done using a dictionary, very similar to specifying parameters for GridSearchCV. Additionally, a computation budget, being the number of sampled candidates or sampling iterations, is specified using the n_iter parameter. For each parameter, either a distribution over possible values or a list of discrete choices (which will be sampled uniformly) can be specified\n",
    "\n",
    "SOURCE:\n",
    "- [Tuning the hyper-parameters of an estimator](http://scikit-learn.org/stable/modules/grid_search.html#grid-search-tips)\n",
    "- [Publication - Random Search for Hyper-Parameter Optimization](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review the pipeline\n",
    "pipe = Pipeline([\n",
    "      ('tfidf', TfidfVectorizer())\n",
    "    , ('lsi', TruncatedSVD(random_state=42))\n",
    "    , ('clf', LogisticRegression(class_weight='balanced', random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a parameter distribution which lists all the possible \n",
    "# hyperparameters to test\n",
    "\n",
    "param_dist = {\n",
    "       #  tfidf hyperparams\n",
    "         'tfidf__max_features': range(200,1000,10)\n",
    "       , 'tfidf__stop_words': [None, 'english']\n",
    "       , 'tfidf__ngram_range': [(1,1),(1,2), (1,3)]\n",
    " \n",
    "       #   lsi hyperparams\n",
    "       ,  'lsi__n_components': range(10,150)\n",
    "      \n",
    "       #   logistic regression hyperparams\n",
    "       ,  'clf__penalty':['l1','l2']\n",
    "       ,  'clf__C':np.linspace(1e3, 10, 100)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...alty='l2', random_state=42,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))]),\n",
       "          fit_params=None, iid=True, n_iter=5, n_jobs=1,\n",
       "          param_distributions={'tfidf__stop_words': [None, 'english'], 'clf__C': array([1000.,  990., ...,   20.,   10.]), 'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)], 'clf__penalty': ['l1', 'l2'], 'lsi__n_components': range(10, 150), 'tfidf__max_features': range(200, 1000, 10)},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit='f1',\n",
       "          return_train_score=True, scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# set cross validatation to test n random hyperparameters\n",
    "grid = RandomizedSearchCV(\n",
    "      pipe\n",
    "    , param_distributions=param_dist\n",
    "    , n_iter=5\n",
    "    , cv=3\n",
    "    , refit='f1'\n",
    "    , scoring='f1'\n",
    "    , return_train_score=True\n",
    ")\n",
    "\n",
    "grid.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_clf__C</th>\n",
       "      <th>param_clf__penalty</th>\n",
       "      <th>param_lsi__n_components</th>\n",
       "      <th>param_tfidf__max_features</th>\n",
       "      <th>param_tfidf__ngram_range</th>\n",
       "      <th>param_tfidf__stop_words</th>\n",
       "      <th>...</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.959695</td>\n",
       "      <td>0.019246</td>\n",
       "      <td>0.385400</td>\n",
       "      <td>0.626470</td>\n",
       "      <td>560</td>\n",
       "      <td>l1</td>\n",
       "      <td>145</td>\n",
       "      <td>440</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262774</td>\n",
       "      <td>0.712934</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.570732</td>\n",
       "      <td>7.333193</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>0.088074</td>\n",
       "      <td>0.061986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.237064</td>\n",
       "      <td>0.020345</td>\n",
       "      <td>0.362088</td>\n",
       "      <td>0.505438</td>\n",
       "      <td>560</td>\n",
       "      <td>l2</td>\n",
       "      <td>98</td>\n",
       "      <td>930</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>english</td>\n",
       "      <td>...</td>\n",
       "      <td>0.330935</td>\n",
       "      <td>0.610811</td>\n",
       "      <td>0.381295</td>\n",
       "      <td>0.464208</td>\n",
       "      <td>0.374101</td>\n",
       "      <td>0.441296</td>\n",
       "      <td>0.005063</td>\n",
       "      <td>0.004114</td>\n",
       "      <td>0.022247</td>\n",
       "      <td>0.075095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.192995</td>\n",
       "      <td>0.009836</td>\n",
       "      <td>0.347888</td>\n",
       "      <td>0.471862</td>\n",
       "      <td>860</td>\n",
       "      <td>l2</td>\n",
       "      <td>82</td>\n",
       "      <td>730</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "      <td>...</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.627778</td>\n",
       "      <td>0.340984</td>\n",
       "      <td>0.421260</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.366548</td>\n",
       "      <td>0.010524</td>\n",
       "      <td>0.001312</td>\n",
       "      <td>0.005657</td>\n",
       "      <td>0.112489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.580011</td>\n",
       "      <td>0.010180</td>\n",
       "      <td>0.311859</td>\n",
       "      <td>0.390505</td>\n",
       "      <td>520</td>\n",
       "      <td>l1</td>\n",
       "      <td>55</td>\n",
       "      <td>590</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "      <td>...</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.481562</td>\n",
       "      <td>0.294618</td>\n",
       "      <td>0.336163</td>\n",
       "      <td>0.340984</td>\n",
       "      <td>0.353791</td>\n",
       "      <td>1.250718</td>\n",
       "      <td>0.001552</td>\n",
       "      <td>0.020700</td>\n",
       "      <td>0.064788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.077677</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>0.300631</td>\n",
       "      <td>0.338420</td>\n",
       "      <td>580</td>\n",
       "      <td>l2</td>\n",
       "      <td>43</td>\n",
       "      <td>760</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342541</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.282667</td>\n",
       "      <td>0.313333</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.292835</td>\n",
       "      <td>0.004030</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>0.029770</td>\n",
       "      <td>0.050668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "2      22.959695         0.019246         0.385400          0.626470   \n",
       "1       0.237064         0.020345         0.362088          0.505438   \n",
       "3       0.192995         0.009836         0.347888          0.471862   \n",
       "4       1.580011         0.010180         0.311859          0.390505   \n",
       "0       0.077677         0.011000         0.300631          0.338420   \n",
       "\n",
       "  param_clf__C param_clf__penalty param_lsi__n_components  \\\n",
       "2          560                 l1                     145   \n",
       "1          560                 l2                      98   \n",
       "3          860                 l2                      82   \n",
       "4          520                 l1                      55   \n",
       "0          580                 l2                      43   \n",
       "\n",
       "  param_tfidf__max_features param_tfidf__ngram_range param_tfidf__stop_words  \\\n",
       "2                       440                   (1, 2)                    None   \n",
       "1                       930                   (1, 3)                 english   \n",
       "3                       730                   (1, 1)                 english   \n",
       "4                       590                   (1, 1)                 english   \n",
       "0                       760                   (1, 1)                    None   \n",
       "\n",
       "        ...        split0_test_score  split0_train_score  split1_test_score  \\\n",
       "2       ...                 0.262774            0.712934           0.465116   \n",
       "1       ...                 0.330935            0.610811           0.381295   \n",
       "3       ...                 0.354839            0.627778           0.340984   \n",
       "4       ...                 0.300000            0.481562           0.294618   \n",
       "0       ...                 0.342541            0.409091           0.282667   \n",
       "\n",
       "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "2            0.595745           0.428571            0.570732      7.333193   \n",
       "1            0.464208           0.374101            0.441296      0.005063   \n",
       "3            0.421260           0.347826            0.366548      0.010524   \n",
       "4            0.336163           0.340984            0.353791      1.250718   \n",
       "0            0.313333           0.276596            0.292835      0.004030   \n",
       "\n",
       "   std_score_time  std_test_score  std_train_score  \n",
       "2        0.003150        0.088074         0.061986  \n",
       "1        0.004114        0.022247         0.075095  \n",
       "3        0.001312        0.005657         0.112489  \n",
       "4        0.001552        0.020700         0.064788  \n",
       "0        0.001634        0.029770         0.050668  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid.cv_results_).sort_values('mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alsherman\\AppData\\Local\\Continuum\\anaconda3\\envs\\guild\\lib\\site-packages\\sklearn\\linear_model\\base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "C:\\Users\\alsherman\\AppData\\Local\\Continuum\\anaconda3\\envs\\guild\\lib\\site-packages\\sklearn\\linear_model\\base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "C:\\Users\\alsherman\\AppData\\Local\\Continuum\\anaconda3\\envs\\guild\\lib\\site-packages\\sklearn\\linear_model\\base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>mean_test_neg_log_loss</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>mean_train_accuracy</th>\n",
       "      <th>mean_train_f1</th>\n",
       "      <th>mean_train_neg_log_loss</th>\n",
       "      <th>...</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>std_test_f1</th>\n",
       "      <th>std_test_neg_log_loss</th>\n",
       "      <th>std_test_precision</th>\n",
       "      <th>std_test_recall</th>\n",
       "      <th>std_train_accuracy</th>\n",
       "      <th>std_train_f1</th>\n",
       "      <th>std_train_neg_log_loss</th>\n",
       "      <th>std_train_precision</th>\n",
       "      <th>std_train_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.095832</td>\n",
       "      <td>0.088355</td>\n",
       "      <td>0.739502</td>\n",
       "      <td>0.252743</td>\n",
       "      <td>-0.572768</td>\n",
       "      <td>0.162800</td>\n",
       "      <td>0.669618</td>\n",
       "      <td>0.769049</td>\n",
       "      <td>0.293649</td>\n",
       "      <td>-0.526689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083643</td>\n",
       "      <td>0.025575</td>\n",
       "      <td>0.081144</td>\n",
       "      <td>0.034418</td>\n",
       "      <td>0.156073</td>\n",
       "      <td>0.022039</td>\n",
       "      <td>0.041570</td>\n",
       "      <td>0.032473</td>\n",
       "      <td>0.027199</td>\n",
       "      <td>0.085289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.281565</td>\n",
       "      <td>0.098219</td>\n",
       "      <td>0.869395</td>\n",
       "      <td>0.389099</td>\n",
       "      <td>-0.493354</td>\n",
       "      <td>0.280947</td>\n",
       "      <td>0.686884</td>\n",
       "      <td>0.904281</td>\n",
       "      <td>0.556191</td>\n",
       "      <td>-0.266683</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019563</td>\n",
       "      <td>0.047906</td>\n",
       "      <td>0.117388</td>\n",
       "      <td>0.002726</td>\n",
       "      <td>0.217637</td>\n",
       "      <td>0.022122</td>\n",
       "      <td>0.064626</td>\n",
       "      <td>0.058364</td>\n",
       "      <td>0.063286</td>\n",
       "      <td>0.027300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.442034</td>\n",
       "      <td>0.097029</td>\n",
       "      <td>0.805338</td>\n",
       "      <td>0.324548</td>\n",
       "      <td>-0.468460</td>\n",
       "      <td>0.219573</td>\n",
       "      <td>0.731802</td>\n",
       "      <td>0.827062</td>\n",
       "      <td>0.396838</td>\n",
       "      <td>-0.369704</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062636</td>\n",
       "      <td>0.022369</td>\n",
       "      <td>0.064617</td>\n",
       "      <td>0.033553</td>\n",
       "      <td>0.211727</td>\n",
       "      <td>0.030561</td>\n",
       "      <td>0.053791</td>\n",
       "      <td>0.052787</td>\n",
       "      <td>0.042836</td>\n",
       "      <td>0.034257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.475482</td>\n",
       "      <td>0.056529</td>\n",
       "      <td>0.865125</td>\n",
       "      <td>0.359567</td>\n",
       "      <td>-0.932131</td>\n",
       "      <td>0.256359</td>\n",
       "      <td>0.636401</td>\n",
       "      <td>0.916741</td>\n",
       "      <td>0.605656</td>\n",
       "      <td>-0.221610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011836</td>\n",
       "      <td>0.075618</td>\n",
       "      <td>0.662773</td>\n",
       "      <td>0.031691</td>\n",
       "      <td>0.230102</td>\n",
       "      <td>0.035074</td>\n",
       "      <td>0.107832</td>\n",
       "      <td>0.087339</td>\n",
       "      <td>0.115691</td>\n",
       "      <td>0.040139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.160038</td>\n",
       "      <td>0.080849</td>\n",
       "      <td>0.816726</td>\n",
       "      <td>0.339725</td>\n",
       "      <td>-0.467869</td>\n",
       "      <td>0.231727</td>\n",
       "      <td>0.737284</td>\n",
       "      <td>0.854643</td>\n",
       "      <td>0.450059</td>\n",
       "      <td>-0.330038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052266</td>\n",
       "      <td>0.011298</td>\n",
       "      <td>0.012328</td>\n",
       "      <td>0.036516</td>\n",
       "      <td>0.191684</td>\n",
       "      <td>0.034154</td>\n",
       "      <td>0.071891</td>\n",
       "      <td>0.075325</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.034132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_accuracy  mean_test_f1  \\\n",
       "0       0.095832         0.088355            0.739502      0.252743   \n",
       "1       0.281565         0.098219            0.869395      0.389099   \n",
       "2       2.442034         0.097029            0.805338      0.324548   \n",
       "3      19.475482         0.056529            0.865125      0.359567   \n",
       "4       0.160038         0.080849            0.816726      0.339725   \n",
       "\n",
       "   mean_test_neg_log_loss  mean_test_precision  mean_test_recall  \\\n",
       "0               -0.572768             0.162800          0.669618   \n",
       "1               -0.493354             0.280947          0.686884   \n",
       "2               -0.468460             0.219573          0.731802   \n",
       "3               -0.932131             0.256359          0.636401   \n",
       "4               -0.467869             0.231727          0.737284   \n",
       "\n",
       "   mean_train_accuracy  mean_train_f1  mean_train_neg_log_loss  \\\n",
       "0             0.769049       0.293649                -0.526689   \n",
       "1             0.904281       0.556191                -0.266683   \n",
       "2             0.827062       0.396838                -0.369704   \n",
       "3             0.916741       0.605656                -0.221610   \n",
       "4             0.854643       0.450059                -0.330038   \n",
       "\n",
       "         ...         std_test_accuracy  std_test_f1 std_test_neg_log_loss  \\\n",
       "0        ...                  0.083643     0.025575              0.081144   \n",
       "1        ...                  0.019563     0.047906              0.117388   \n",
       "2        ...                  0.062636     0.022369              0.064617   \n",
       "3        ...                  0.011836     0.075618              0.662773   \n",
       "4        ...                  0.052266     0.011298              0.012328   \n",
       "\n",
       "  std_test_precision std_test_recall std_train_accuracy std_train_f1  \\\n",
       "0           0.034418        0.156073           0.022039     0.041570   \n",
       "1           0.002726        0.217637           0.022122     0.064626   \n",
       "2           0.033553        0.211727           0.030561     0.053791   \n",
       "3           0.031691        0.230102           0.035074     0.107832   \n",
       "4           0.036516        0.191684           0.034154     0.071891   \n",
       "\n",
       "  std_train_neg_log_loss std_train_precision  std_train_recall  \n",
       "0               0.032473            0.027199          0.085289  \n",
       "1               0.058364            0.063286          0.027300  \n",
       "2               0.052787            0.042836          0.034257  \n",
       "3               0.087339            0.115691          0.040139  \n",
       "4               0.075325            0.061867          0.034132  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set several evaluation metrics to collect during RandomizedSearch\n",
    "scoring = ['accuracy','neg_log_loss','precision','recall','f1']\n",
    "\n",
    "grid = RandomizedSearchCV(\n",
    "      pipe\n",
    "    , param_distributions=param_dist\n",
    "    , n_iter=5\n",
    "    , cv=3\n",
    "    , refit='f1'\n",
    "    , scoring=scoring  # track multiple evaluation metrics\n",
    "    , return_train_score=True\n",
    ")\n",
    "\n",
    "grid.fit(X,y)\n",
    "pd.DataFrame(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add distributions for RandomizedGridSearch hyperparameters sampling\n",
    "\n",
    "##### scipy.stats.distributions\n",
    "\n",
    "[e.g. scipy.stats.lognorm](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.lognorm.html#scipy.stats.lognorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import lognorm\n",
    "dist = np.linspace(lognorm.ppf(0.01, 1), lognorm.ppf(0.99, 1), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x255d022c6d8>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x255d022c438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(dist, lognorm.pdf(dist, 1), 'r-', lw=5, alpha=0.6, label='lognorm pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>param_clf__C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.126112</td>\n",
       "      <td>8.74799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.129439</td>\n",
       "      <td>5.44827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.126390</td>\n",
       "      <td>0.260099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.126112</td>\n",
       "      <td>9.06273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.129439</td>\n",
       "      <td>5.09292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.129278</td>\n",
       "      <td>3.3466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.129278</td>\n",
       "      <td>4.25022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.129278</td>\n",
       "      <td>3.74257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.129278</td>\n",
       "      <td>3.71211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.129278</td>\n",
       "      <td>3.75272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score param_clf__C\n",
       "0         0.126112      8.74799\n",
       "1         0.129439      5.44827\n",
       "2         0.126390     0.260099\n",
       "3         0.126112      9.06273\n",
       "4         0.129439      5.09292\n",
       "5         0.129278       3.3466\n",
       "6         0.129278      4.25022\n",
       "7         0.129278      3.74257\n",
       "8         0.129278      3.71211\n",
       "9         0.129278      3.75272"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scipy.stats.lognorm\n",
    "param_dist = {'clf__C':dist}\n",
    "\n",
    "grid = RandomizedSearchCV(\n",
    "      pipe\n",
    "    , param_distributions=param_dist\n",
    "    , n_iter=10\n",
    "    , cv=2\n",
    "    , scoring='f1'\n",
    "    , return_train_score=True\n",
    ")\n",
    "\n",
    "grid.fit(X,y)\n",
    "\n",
    "# selected eval columns\n",
    "eval_cols = ['mean_test_score','param_clf__C']\n",
    "pd.DataFrame(grid.cv_results_)[eval_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RandomizedGridSearch Robustness to failure with error_score\n",
    "\n",
    "Some parameter settings may result in a failure to fit one or more folds of the data. By default, this will cause the entire search to fail, even if some parameter settings could be fully evaluated. \n",
    "\n",
    "**Setting error_score=0** (or =np.NaN) will make the procedure robust to such failure, issuing a warning and setting the score for that fold to 0 (or NaN), but completing the search.\n",
    "\n",
    "NOTE: Without setting this parameter a single hyperparameter mismatch could cause an entire training routine to fail. For example, if you set a range of max_features with the lowest value having a lower option than another hyperparameter such as lsi or pca components. \n",
    "\n",
    "Speaking from personal experience, I have had 8-hour training routines fail in the last hour due to this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alsherman\\AppData\\Local\\Continuum\\anaconda3\\envs\\guild\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:479: FitFailedWarning: Classifier fit failed. The score on this train-test partition for these parameters will be set to 0.000000. Details: \n",
      "ValueError('n_components must be < n_features; got 110 >= 100',)\n",
      "  \"Details: \\n%r\" % (error_score, e), FitFailedWarning)\n",
      "C:\\Users\\alsherman\\AppData\\Local\\Continuum\\anaconda3\\envs\\guild\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:479: FitFailedWarning: Classifier fit failed. The score on this train-test partition for these parameters will be set to 0.000000. Details: \n",
      "ValueError('n_components must be < n_features; got 110 >= 100',)\n",
      "  \"Details: \\n%r\" % (error_score, e), FitFailedWarning)\n",
      "C:\\Users\\alsherman\\AppData\\Local\\Continuum\\anaconda3\\envs\\guild\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:479: FitFailedWarning: Classifier fit failed. The score on this train-test partition for these parameters will be set to 0.000000. Details: \n",
      "ValueError('n_components must be < n_features; got 120 >= 100',)\n",
      "  \"Details: \\n%r\" % (error_score, e), FitFailedWarning)\n",
      "C:\\Users\\alsherman\\AppData\\Local\\Continuum\\anaconda3\\envs\\guild\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:479: FitFailedWarning: Classifier fit failed. The score on this train-test partition for these parameters will be set to 0.000000. Details: \n",
      "ValueError('n_components must be < n_features; got 120 >= 100',)\n",
      "  \"Details: \\n%r\" % (error_score, e), FitFailedWarning)\n",
      "C:\\Users\\alsherman\\AppData\\Local\\Continuum\\anaconda3\\envs\\guild\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:479: FitFailedWarning: Classifier fit failed. The score on this train-test partition for these parameters will be set to 0.000000. Details: \n",
      "ValueError('n_components must be < n_features; got 120 >= 120',)\n",
      "  \"Details: \\n%r\" % (error_score, e), FitFailedWarning)\n",
      "C:\\Users\\alsherman\\AppData\\Local\\Continuum\\anaconda3\\envs\\guild\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:479: FitFailedWarning: Classifier fit failed. The score on this train-test partition for these parameters will be set to 0.000000. Details: \n",
      "ValueError('n_components must be < n_features; got 120 >= 120',)\n",
      "  \"Details: \\n%r\" % (error_score, e), FitFailedWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_lsi__n_components</th>\n",
       "      <th>param_tfidf__max_features</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.034999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>110</td>\n",
       "      <td>100</td>\n",
       "      <td>{'tfidf__max_features': 100, 'lsi__n_component...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.247999</td>\n",
       "      <td>0.016251</td>\n",
       "      <td>0.3311</td>\n",
       "      <td>0.394646</td>\n",
       "      <td>110</td>\n",
       "      <td>120</td>\n",
       "      <td>{'tfidf__max_features': 120, 'lsi__n_component...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.351562</td>\n",
       "      <td>0.425656</td>\n",
       "      <td>0.310638</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.111499</td>\n",
       "      <td>0.001749</td>\n",
       "      <td>0.020462</td>\n",
       "      <td>0.03101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.025251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>{'tfidf__max_features': 100, 'lsi__n_component...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.046746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>{'tfidf__max_features': 120, 'lsi__n_component...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       0.034999         0.000000           0.0000          0.000000   \n",
       "1       0.247999         0.016251           0.3311          0.394646   \n",
       "2       0.025251         0.000000           0.0000          0.000000   \n",
       "3       0.046746         0.000000           0.0000          0.000000   \n",
       "\n",
       "  param_lsi__n_components param_tfidf__max_features  \\\n",
       "0                     110                       100   \n",
       "1                     110                       120   \n",
       "2                     120                       100   \n",
       "3                     120                       120   \n",
       "\n",
       "                                              params  rank_test_score  \\\n",
       "0  {'tfidf__max_features': 100, 'lsi__n_component...                2   \n",
       "1  {'tfidf__max_features': 120, 'lsi__n_component...                1   \n",
       "2  {'tfidf__max_features': 100, 'lsi__n_component...                2   \n",
       "3  {'tfidf__max_features': 120, 'lsi__n_component...                2   \n",
       "\n",
       "   split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0           0.000000            0.000000           0.000000   \n",
       "1           0.351562            0.425656           0.310638   \n",
       "2           0.000000            0.000000           0.000000   \n",
       "3           0.000000            0.000000           0.000000   \n",
       "\n",
       "   split1_train_score  std_fit_time  std_score_time  std_test_score  \\\n",
       "0            0.000000      0.003999        0.000000        0.000000   \n",
       "1            0.363636      0.111499        0.001749        0.020462   \n",
       "2            0.000000      0.000248        0.000000        0.000000   \n",
       "3            0.000000      0.004251        0.000000        0.000000   \n",
       "\n",
       "   std_train_score  \n",
       "0          0.00000  \n",
       "1          0.03101  \n",
       "2          0.00000  \n",
       "3          0.00000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "      ('tfidf', TfidfVectorizer())\n",
    "    , ('lsi', TruncatedSVD(random_state=42))\n",
    "    , ('clf', LogisticRegression(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "# set a parameter distribution which lists all the possible hyperparameters to test\n",
    "param_dist = {\n",
    "       #  tfidf hyperparams\n",
    "         'tfidf__max_features': [100,120]\n",
    "       #   lsi hyperparams\n",
    "       ,  'lsi__n_components': [110, 120]\n",
    "}\n",
    "\n",
    "grid = RandomizedSearchCV(\n",
    "      pipe\n",
    "    , param_distributions=param_dist\n",
    "    , n_iter=4\n",
    "    , cv=2\n",
    "    , scoring='f1'\n",
    "    , return_train_score=True\n",
    "    , error_score=0\n",
    ")\n",
    "\n",
    "grid.fit(X,y)\n",
    "pd.DataFrame(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested cross-validation (CV)\n",
    "\n",
    "Nested cross-validation (CV) is often used to train a model in which hyperparameters also need to be optimized. Nested CV estimates the generalization error of the underlying model and its (hyper)parameter search. Choosing the parameters that maximize non-nested CV biases the model to the dataset, yielding an overly-optimistic score.\n",
    "\n",
    "Model selection without nested CV uses the same data to tune model parameters and evaluate model performance. Information may thus “leak” into the model and overfit the data. The magnitude of this effect is primarily dependent on the size of the dataset and the stability of the model.\n",
    "\n",
    "To avoid this problem, nested CV effectively uses a series of train/validation/test set splits. In the inner loop (here executed by GridSearchCV), the score is approximately maximized by fitting a model to each training set, and then directly maximized in selecting (hyper)parameters over the validation set. In the outer loop (here in cross_val_score), generalization error is estimated by averaging test set scores over several dataset splits.\n",
    "\n",
    "SOURCE:\n",
    "- [Nested versus non-nested cross validation](http://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html#sphx-glr-auto-examples-model-selection-plot-nested-cross-validation-iris-py)\n",
    "- [Nested Cross Validation: When Cross Validation Isn’t Enough](https://www.elderresearch.com/blog/nested-cross-validation)\n",
    "- [Nested Cross Validation - sklearn](https://chrisalbon.com/machine_learning/model_evaluation/nested_cross_validation/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3464988330767533"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "param_dist = {\n",
    "    'tfidf__max_features': [150]\n",
    "  , 'lsi__n_components': [110, 120, 130]\n",
    "}\n",
    "\n",
    "grid = RandomizedSearchCV(\n",
    "      pipe\n",
    "    , param_distributions=param_dist\n",
    "    , n_iter=3\n",
    "    , cv=2\n",
    "    , scoring='f1'\n",
    "    , return_train_score=True\n",
    ")\n",
    "\n",
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(grid, X=X, y=y, cv=3)\n",
    "nested_score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FeatureUnion combines several transformer objects into a new transformer that combines their output. A FeatureUnion takes a list of transformer objects. During fitting, each of these is fit to the data independently. For transforming data, the transformers are applied in parallel, and the sample vectors they output are concatenated end-to-end into larger vectors.\n",
    "\n",
    "FeatureUnion serves the same purposes as Pipeline - convenience and joint parameter estimation and validation.\n",
    "\n",
    "FeatureUnion and Pipeline can be combined to create complex models.\n",
    "\n",
    "(A FeatureUnion has no way of checking whether two transformers might produce identical features. It only produces a union when the feature sets are disjoint, and making sure they are the caller’s responsibility.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('tfidf_pipe', Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_ra...uncatedSVD(algorithm='randomized', n_components=100, n_iter=5,\n",
       "       random_state=42, tol=0.0))]))],\n",
       "       transformer_weights=None)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "tfidf_pipe = Pipeline([\n",
    "      ('tfidf', TfidfVectorizer())\n",
    "    , ('lsi', TruncatedSVD(n_components=100, random_state=42))\n",
    "])\n",
    "\n",
    "count_pipe = Pipeline([\n",
    "      ('count', CountVectorizer())\n",
    "    , ('lsi', TruncatedSVD(n_components=100, random_state=42))\n",
    "])\n",
    "\n",
    "FeatureUnion([('tfidf_pipe', tfidf_pipe), ('count_pipe',count_pipe)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2707553122742996"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add a feature union to a pipeline\n",
    "pipe = Pipeline([\n",
    "      ('feature_union', FeatureUnion([\n",
    "          ('tfidf_pipe', tfidf_pipe)\n",
    "        , ('count_pipe',count_pipe)\n",
    "    ]))\n",
    "    , ('clf', LogisticRegression(C=1, random_state=42))\n",
    "])\n",
    "\n",
    "param_dist = {\n",
    "    'feature_union__count_pipe__count__max_features': [200,500,1000,5000]\n",
    "  , 'feature_union__tfidf_pipe__tfidf__max_features': [200,500,1000,5000]\n",
    "}\n",
    "\n",
    "grid = RandomizedSearchCV(\n",
    "      pipe\n",
    "    , param_distributions=param_dist\n",
    "    , n_iter=3\n",
    "    , cv=2\n",
    "    , scoring='f1'\n",
    "    , return_train_score=True\n",
    ")\n",
    "\n",
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(grid, X=X, y=y, cv=3)\n",
    "nested_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3376104417670683"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add a feature union to a pipeline\n",
    "# provide different weights to each step in the pipeline\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('feature_union', FeatureUnion([\n",
    "          ('tfidf_pipe', tfidf_pipe)\n",
    "        , ('count_pipe', count_pipe)\n",
    "    # add transformer weights\n",
    "    ], transformer_weights={\n",
    "          \"tfidf_pipe\": 5\n",
    "        , \"count_pipe\": 1\n",
    "        }))\n",
    "    , ('clf', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "grid = RandomizedSearchCV(\n",
    "      pipe\n",
    "    , param_distributions=param_dist\n",
    "    , n_iter=3\n",
    "    , cv=2\n",
    "    , scoring='f1'\n",
    "    , return_train_score=True\n",
    ")\n",
    "\n",
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(grid, X=X, y=y, cv=3)\n",
    "nested_score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>section_name</th>\n",
       "      <th>section_text</th>\n",
       "      <th>criteria</th>\n",
       "      <th>section_length</th>\n",
       "      <th>contains_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>southwest-airlines-co_annual_report_1994.docx</td>\n",
       "      <td>FIRST SECTION</td>\n",
       "      <td>© 1994 Southwest Airlines Co. This annual repo...</td>\n",
       "      <td>&lt;function heading at 0x000001D4AA492EA0&gt;</td>\n",
       "      <td>83</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   section_id                                       filename   section_name  \\\n",
       "0           1  southwest-airlines-co_annual_report_1994.docx  FIRST SECTION   \n",
       "\n",
       "                                        section_text  \\\n",
       "0  © 1994 Southwest Airlines Co. This annual repo...   \n",
       "\n",
       "                                   criteria  section_length  contains_fee  \n",
       "0  <function heading at 0x000001D4AA492EA0>              83         False  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update X to include both text fields\n",
    "X = df[['section_name','section_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ItemSelector\n",
    "Create a custom transformer to select a subset of features inside a pipeline\n",
    "\n",
    "Transformers must have a fit and transform method: \n",
    "\n",
    "**fit method:** used to learn any parameters from the traning data that will be applied to the testing data. As an example, learning the mean or standard deviation in a StandardScaler or learning the vocabulary in a CountVectorizer. If the transformer does not need to learn any parameters, then the fit must still exist (as required to keep the scikit-learn api consistent), but can simple have a return self as the entire code.\n",
    "\n",
    "**transform:** apply any transformations to the data, such as applying a z-score stanardization in StandardScaler, filtering to relevant terminology in a CountVectoirzer, or selecting a subset of features in the below ItemSelector\n",
    "\n",
    "**init method:** optional - used to store any parameters needed for other methods in the transformer, such as storing the variable names used to select the subset of features in the below ItemSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE: scikit-learn.org/stable/auto_examples/hetero_feature_union.html\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Select a subset of features as a step in a sklearn pipeline \"\"\"\n",
    "\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "    \n",
    "class tokenizer(....):\n",
    "    \n",
    "    def __init__(self, hyperparams):\n",
    "        tokenizer do;\n",
    "        self.terms = tokenizer..random_state=42\n",
    "        self.text\n",
    "    \n",
    "    def fit:\n",
    "        self.text = tokenizer text\n",
    "        data['value'] = value\n",
    "    def transform\n",
    "        return {'text':self.text, 'thing2':thing_two}\n",
    "    \n",
    "data = {}\n",
    "    \n",
    "Pipeline(step1,step2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12654491041646013"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example pipeline to select only a single text field\n",
    "\n",
    "pipe = Pipeline([\n",
    "       ('select_text', ItemSelector(key='section_name'))\n",
    "    ,  ('tfidf', TfidfVectorizer())\n",
    "    ,  ('lsi', TruncatedSVD(random_state=42))\n",
    "    ,  ('clf', LogisticRegression(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "param_dist = {'tfidf__max_features': [200,500,1000,5000]}\n",
    "\n",
    "grid = RandomizedSearchCV(\n",
    "      pipe\n",
    "    , param_distributions=param_dist\n",
    "    , n_iter=3\n",
    "    , cv=2\n",
    "    , scoring='f1'\n",
    "    , return_train_score=True\n",
    ")\n",
    "\n",
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(grid, X=X, y=y, cv=3)\n",
    "nested_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5230770870126068"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a pipe to apply separate preprocessing to different text fields\n",
    "# create 50 components for the section name and 100 for the section text\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('feature_union', FeatureUnion([\n",
    "          ('section_name', Pipeline([\n",
    "               ('select_text', ItemSelector(key='section_name'))\n",
    "            ,  ('tfidf', TfidfVectorizer())\n",
    "            ,  ('lsi', TruncatedSVD(n_components=50, random_state=42))\n",
    "          ])),\n",
    "          ('section_text', Pipeline([\n",
    "               ('select_text', ItemSelector(key='section_text'))\n",
    "            ,  ('tfidf', TfidfVectorizer())\n",
    "            ,  ('lsi', TruncatedSVD(n_components=100, random_state=42))\n",
    "          ]))\n",
    "        ])\n",
    "    )\n",
    "    , ('clf', LogisticRegression(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "param_dist = {'feature_union__section_name__tfidf__max_features': [200,500,1000,5000]}\n",
    "\n",
    "grid = RandomizedSearchCV(\n",
    "      pipe\n",
    "    , param_distributions=param_dist\n",
    "    , n_iter=3\n",
    "    , cv=2\n",
    "    , scoring='f1'\n",
    "    , return_train_score=True\n",
    ")\n",
    "\n",
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(grid, X=X, y=y, cv=3)\n",
    "nested_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update X to include both text fields\n",
    "X = df[['section_name','section_length']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparseMatrixTransformer\n",
    "\n",
    "Transform numeric data into a sparse representation to allow numeric and text fields to be combined together in a pipeline with a FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "class SparseMatrixTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Converts a dense matrix into a sparse matrix \"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # convert dense feature(s) into a sparse matrix\n",
    "        return sp.sparse.csr_matrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42523768023298514"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a feature union that includes a numeric feature\n",
    "# in order to combine numeric and text features, you must convert\n",
    "# the dense and sparse matrices to match\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('feature_union', FeatureUnion([\n",
    "          ('text', Pipeline([\n",
    "               ('select_text', ItemSelector(key='section_name'))\n",
    "            ,  ('tfidf', TfidfVectorizer())\n",
    "            ,  ('lsi', TruncatedSVD(n_components=100, random_state=42))\n",
    "          ]))\n",
    "        , ('numeric', Pipeline([\n",
    "               ('select_len', ItemSelector(key=['section_length']))\n",
    "             , ('sparse', SparseMatrixTransformer())\n",
    "          ]))\n",
    "        ])\n",
    "    )\n",
    "    , ('clf', LogisticRegression(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "param_dist = {'feature_union__text__tfidf__max_features': [200,500,1000,5000]}\n",
    "\n",
    "grid = RandomizedSearchCV(\n",
    "      pipe\n",
    "    , param_distributions=param_dist\n",
    "    , n_iter=3\n",
    "    , cv=2\n",
    "    , scoring='f1'\n",
    "    , return_train_score=True\n",
    ")\n",
    "\n",
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(grid, X=X, y=y, cv=3)\n",
    "nested_score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DummyTransformer\n",
    "\n",
    "Transform data to create dummy variables. At times, the number of dummy fields may differ among the training splits. A DummyTransformer ensures that the dummy variables are learned in each training set as opposed to before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['section_name','section_text','criteria']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, default_value=0):\n",
    "        self.dummy_cols = None\n",
    "        self.default = default_value \n",
    "            \n",
    "    def fit(self, X, y=None):\n",
    "        # identify the dummy fields from the training data\n",
    "        self.dummy_cols = pd.get_dummies(X).columns\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # collect the dummy columns from testing data\n",
    "        X = pd.get_dummies(X)\n",
    "        # add missing features learned in training data\n",
    "        for col in self.dummy_cols:\n",
    "            if col not in X.columns:\n",
    "                X[col] = self.default\n",
    "                \n",
    "        # only retain dummy fields that existed in the training data\n",
    "        return X[self.dummy_cols]\n",
    "\n",
    "    # add a method to review the learned dummy fields\n",
    "    def get_feature_names(self):\n",
    "        return self.dummy_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19448082279088855\n",
      "Wall time: 893 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create a feature union that includes a numeric feature\n",
    "# in order to combine numeric and text features, you must convert\n",
    "# the dense and sparse matrices to match\n",
    "\n",
    "pipe = Pipeline([\n",
    "      ('criteria', ItemSelector(key=['criteria']))\n",
    "    , ('dummy', DummyTransformer(default_value=1))\n",
    "    , ('clf', LogisticRegression(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "dist = np.linspace(lognorm.ppf(0.01, 1), lognorm.ppf(0.99, 1), 1000)\n",
    "param_dist = {'clf__C': dist}\n",
    "\n",
    "grid = RandomizedSearchCV(\n",
    "      pipe\n",
    "    , param_distributions=param_dist\n",
    "    , n_iter=5\n",
    "    , cv=2\n",
    "    , scoring='f1'\n",
    "    , return_train_score=True\n",
    ")\n",
    "\n",
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(grid, X=X, y=y, cv=3)\n",
    "print(nested_score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['section_name','section_text','criteria','section_length']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55122316233209\n",
      "Wall time: 40.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('feature_union', FeatureUnion([\n",
    "          ('dummy', Pipeline([\n",
    "              ('dummy_variables', ItemSelector(key=['criteria']))\n",
    "            , ('dummy', DummyTransformer())\n",
    "          ])),\n",
    "          ('numeric', Pipeline([\n",
    "               ('select_len', ItemSelector(key=['section_length']))\n",
    "             , ('sparse', SparseMatrixTransformer())\n",
    "          ])),\n",
    "          ('section_name', Pipeline([\n",
    "               ('select_text', ItemSelector(key='section_name'))\n",
    "            ,  ('tfidf', TfidfVectorizer())\n",
    "            ,  ('lsi', TruncatedSVD(n_components=50, random_state=42))\n",
    "          ])),\n",
    "          ('section_text', Pipeline([\n",
    "               ('select_text', ItemSelector(key='section_text'))\n",
    "            ,  ('tfidf', TfidfVectorizer())\n",
    "            ,  ('lsi', TruncatedSVD(n_components=100, random_state=42))\n",
    "          ]))\n",
    "        ])\n",
    "    )\n",
    "    , ('clf', LogisticRegression(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "param_dist = {'feature_union__section_name__tfidf__max_features': [200,500,1000,5000]}\n",
    "\n",
    "grid = RandomizedSearchCV(\n",
    "      pipe\n",
    "    , param_distributions=param_dist\n",
    "    , n_iter=3\n",
    "    , cv=2\n",
    "    , scoring='f1'\n",
    "    , return_train_score=True\n",
    ")\n",
    "\n",
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(grid, X=X, y=y, cv=3)\n",
    "print(nested_score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline optimizations - cache intermediate steps with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['section_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regular Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# regular pipeline\n",
    "pipe = Pipeline([\n",
    "      ('tfidf', TfidfVectorizer())\n",
    "    , ('lsi', TruncatedSVD(random_state=42))\n",
    "    , ('clf', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# hyperparameters\n",
    "dist = np.linspace(lognorm.ppf(0.01, 1), lognorm.ppf(0.99, 1), 1000)\n",
    "param_dist = {\n",
    "         'tfidf__max_features': range(200,1000,10)\n",
    "       , 'tfidf__stop_words': [None, 'english']\n",
    "       , 'tfidf__ngram_range': [(1,1),(1,2), (1,3)]\n",
    "       , 'lsi__n_components': range(10,150)\n",
    "       , 'clf__penalty':['l1','l2']\n",
    "       , 'clf__C':dist\n",
    "       , 'clf__class_weight':['balanced']\n",
    "}\n",
    "\n",
    "# use cross validation\n",
    "grid = RandomizedSearchCV(\n",
    "      pipe\n",
    "    , param_distributions=param_dist\n",
    "    , n_iter=1000\n",
    "    , cv=3\n",
    "    , refit='f1'\n",
    "    , scoring='f1'\n",
    "    , error_score=0\n",
    ")\n",
    "\n",
    "grid.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pipeline with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tempdir: C:\\Users\\ALSHER~1\\AppData\\Local\\Temp\\tmpahgmr9gg\n"
     ]
    }
   ],
   "source": [
    "from tempfile import mkdtemp\n",
    "\n",
    "# create a temp dir to store fit data from sklearn pipeline\n",
    "cachedir = mkdtemp()\n",
    "print('tempdir: {}'.format(cachedir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# add memory=cachedir to save fitting in intermediate steps \n",
    "# (e.g. fit tfidf and lsi only once )\n",
    "pipe = Pipeline([\n",
    "      ('tfidf', TfidfVectorizer())\n",
    "    , ('lsi', TruncatedSVD(random_state=42))\n",
    "    , ('clf', LogisticRegression(random_state=42))\n",
    "], memory=cachedir)\n",
    "\n",
    "# hyperparameters\n",
    "dist = np.linspace(lognorm.ppf(0.01, 1), lognorm.ppf(0.99, 1), 1000)\n",
    "param_dist = {\n",
    "         'tfidf__max_features': range(200,1000,10)\n",
    "       , 'tfidf__stop_words': [None, 'english']\n",
    "       , 'tfidf__ngram_range': [(1,1),(1,2), (1,3)]\n",
    "       , 'lsi__n_components': range(10,150)\n",
    "       , 'clf__penalty':['l1','l2']\n",
    "       , 'clf__C':dist\n",
    "       , 'clf__class_weight':['balanced']\n",
    "}\n",
    "\n",
    "# use cross validation\n",
    "grid = RandomizedSearchCV(\n",
    "      pipe\n",
    "    , param_distributions=param_dist\n",
    "    , n_iter=1000\n",
    "    , cv=3\n",
    "    , refit='f1'\n",
    "    , scoring='f1'\n",
    "    , error_score=0\n",
    ")\n",
    "\n",
    "grid.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the cache directory when you don't need it anymore\n",
    "rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Persistence\n",
    "\n",
    "After training a scikit-learn model, it is desirable to have a way to persist the model for future use without having to retrain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['filename.pkl']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(grid, 'filename.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = joblib.load('filename.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Security & maintainability limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle (and joblib by extension), has some issues regarding maintainability and security. Because of this,\n",
    "\n",
    "Never unpickle untrusted data as it could lead to malicious code being executed upon loading.\n",
    "While models saved using one version of scikit-learn might load in other versions, this is entirely unsupported and inadvisable. It should also be kept in mind that operations performed on such data could give different and unexpected results.\n",
    "In order to rebuild a similar model with future versions of scikit-learn, additional metadata should be saved along the pickled model:\n",
    "\n",
    "The training data, e.g. a reference to a immutable snapshot\n",
    "The python source code used to generate the model\n",
    "The versions of scikit-learn and its dependencies\n",
    "The cross validation score obtained on the training data\n",
    "This should make it possible to check that the cross-validation score is in the same range as before.\n",
    "\n",
    "Since a model internal representation may be different on two different architectures, dumping a model on one architecture and loading it on another architecture is not supported.\n",
    "\n",
    "If you want to know more about these issues and explore other possible serialization methods, please refer to [this talk by Alex Gaynor - pickles are for delis not software](http://pyvideo.org/pycon-us-2014/pickles-are-for-delis-not-software.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
