{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing Exercise\n",
    "\n",
    "##### Author: Alex Sherman | alsherman@deloitte.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agenda\n",
    "\n",
    "1. SpaCy\n",
    "2. Text Tokenization, POS Tagging, Parsing, NER\n",
    "3. Text Rule-based matching\n",
    "4. Text Pipelines\n",
    "5. Advanced SpaCy Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import Image\n",
    "from configparser import ConfigParser, ExtendedInterpolation\n",
    "\n",
    "config = ConfigParser(interpolation=ExtendedInterpolation())\n",
    "config.read('../../config.ini')\n",
    "DB_PATH = config['DATABASES']['PROJECT_DB_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sqlite:///C:\\\\Users\\\\alsherman\\\\Desktop\\\\PycharmProjects\\\\firm_initiatives\\\\ml_guild\\\\raw_data\\\\databases\\\\annual_report.db'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm DB_PATH is in the correct db directory, otherwise the rest of the code will not work\n",
    "DB_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DOCUMENTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SECTIONS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name\n",
       "0  DOCUMENTS\n",
       "1   SECTIONS"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for the names of the tables in the database\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine(DB_PATH)\n",
    "pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table'\", con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>path</th>\n",
       "      <th>filename</th>\n",
       "      <th>year</th>\n",
       "      <th>document_text</th>\n",
       "      <th>table_text</th>\n",
       "      <th>author</th>\n",
       "      <th>last_modified_by</th>\n",
       "      <th>created</th>\n",
       "      <th>revision</th>\n",
       "      <th>num_tables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>C:\\Users\\alsherman\\Desktop\\PycharmProjects\\fir...</td>\n",
       "      <td>southwest-airlines-co_annual_report_1994.docx</td>\n",
       "      <td>1994</td>\n",
       "      <td>© 1994 Southwest Airlines Co. This annual repo...</td>\n",
       "      <td>1994 1993 Percent Change Operating revenues $...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2018-01-03 22:40:27</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>C:\\Users\\alsherman\\Desktop\\PycharmProjects\\fir...</td>\n",
       "      <td>southwest-airlines-co_annual_report_1995.docx</td>\n",
       "      <td>1995</td>\n",
       "      <td>Southwest Airlines Co. 1995 Annual Report OUR ...</td>\n",
       "      <td>1995 1994 Percent Change Percent Change Opera...</td>\n",
       "      <td>43945</td>\n",
       "      <td></td>\n",
       "      <td>2018-01-03 22:40:43</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>C:\\Users\\alsherman\\Desktop\\PycharmProjects\\fir...</td>\n",
       "      <td>southwest-airlines-co_annual_report_1996.docx</td>\n",
       "      <td>1996</td>\n",
       "      <td>Consolidated Highlights\\t2 Introduction\\t3 Let...</td>\n",
       "      <td>CONSOLIDATED HIGHLIGHTS    (DOLLARS IN THOUSAN...</td>\n",
       "      <td>23133</td>\n",
       "      <td></td>\n",
       "      <td>2018-01-03 22:40:58</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document_id                                               path  \\\n",
       "0            1  C:\\Users\\alsherman\\Desktop\\PycharmProjects\\fir...   \n",
       "1            2  C:\\Users\\alsherman\\Desktop\\PycharmProjects\\fir...   \n",
       "2            3  C:\\Users\\alsherman\\Desktop\\PycharmProjects\\fir...   \n",
       "\n",
       "                                        filename  year  \\\n",
       "0  southwest-airlines-co_annual_report_1994.docx  1994   \n",
       "1  southwest-airlines-co_annual_report_1995.docx  1995   \n",
       "2  southwest-airlines-co_annual_report_1996.docx  1996   \n",
       "\n",
       "                                       document_text  \\\n",
       "0  © 1994 Southwest Airlines Co. This annual repo...   \n",
       "1  Southwest Airlines Co. 1995 Annual Report OUR ...   \n",
       "2  Consolidated Highlights\\t2 Introduction\\t3 Let...   \n",
       "\n",
       "                                          table_text author last_modified_by  \\\n",
       "0   1994 1993 Percent Change Operating revenues $...                           \n",
       "1   1995 1994 Percent Change Percent Change Opera...  43945                    \n",
       "2  CONSOLIDATED HIGHLIGHTS    (DOLLARS IN THOUSAN...  23133                    \n",
       "\n",
       "               created  revision  num_tables  \n",
       "0  2018-01-03 22:40:27         0          24  \n",
       "1  2018-01-03 22:40:43         0          32  \n",
       "2  2018-01-03 22:40:58         0          24  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the oracle 10k documents \n",
    "doc_df = pd.read_sql(\"SELECT * FROM Documents\", con=engine)\n",
    "doc_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>section_name</th>\n",
       "      <th>section_text</th>\n",
       "      <th>criteria</th>\n",
       "      <th>section_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>southwest-airlines-co_annual_report_1994.docx</td>\n",
       "      <td>FIRST SECTION</td>\n",
       "      <td>© 1994 Southwest Airlines Co. This annual repo...</td>\n",
       "      <td>&lt;function heading at 0x000001D4AA492EA0&gt;</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>southwest-airlines-co_annual_report_1994.docx</td>\n",
       "      <td>TABLE OF CONTENTS CONSOLIDATED HIGHLIGHTS</td>\n",
       "      <td>(Dollars in thousands except per share amounts...</td>\n",
       "      <td>&lt;function heading at 0x000001D4AA492EA0&gt;</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>southwest-airlines-co_annual_report_1994.docx</td>\n",
       "      <td>NET INCOME NET INCOME PER SHARE LOW FARES</td>\n",
       "      <td>Southwest Airlines was built, from the ground ...</td>\n",
       "      <td>&lt;function heading at 0x000001D4AA492EA0&gt;</td>\n",
       "      <td>1553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   section_id                                       filename  \\\n",
       "0           1  southwest-airlines-co_annual_report_1994.docx   \n",
       "1           2  southwest-airlines-co_annual_report_1994.docx   \n",
       "2           3  southwest-airlines-co_annual_report_1994.docx   \n",
       "\n",
       "                                section_name  \\\n",
       "0                              FIRST SECTION   \n",
       "1  TABLE OF CONTENTS CONSOLIDATED HIGHLIGHTS   \n",
       "2  NET INCOME NET INCOME PER SHARE LOW FARES   \n",
       "\n",
       "                                        section_text  \\\n",
       "0  © 1994 Southwest Airlines Co. This annual repo...   \n",
       "1  (Dollars in thousands except per share amounts...   \n",
       "2  Southwest Airlines was built, from the ground ...   \n",
       "\n",
       "                                   criteria  section_length  \n",
       "0  <function heading at 0x000001D4AA492EA0>              83  \n",
       "1  <function heading at 0x000001D4AA492EA0>             113  \n",
       "2  <function heading at 0x000001D4AA492EA0>            1553  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the oracle 10k sections\n",
    "df = pd.read_sql(\"SELECT * FROM Sections \", con=engine)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "1. Create a dataframe named fees_df that stores the section_name for each section that includes the word fee in the section_text\n",
    "2. Print the count of matched sections\n",
    "3. Print the first five matched sections (print the section_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create fees_df\n",
    "\n",
    "\n",
    "# print the count of matches\n",
    "\n",
    "\n",
    "# view the first five section names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A complaint alleging violations of federal antitrust laws and seeking certification as a class action was filed against Delta Air Lines, Inc. and AirTran in the United States District Court for the Northern District of Georgia in Atlanta on May 22, 2009. The complaint alleged, among other things, that AirTran attempted to monopolize air travel in violation of Section 2 of the Sherman Act, and conspired with Delta in imposing $15-per-bag fees for the first item of checked luggage in violation of Section 1 of the Sherman Act. The initial complaint sought treble damages on behalf of a putative class of persons or entities in the United States who directly paid Delta and/or AirTran such fees on domestic flights beginning December 5, 2008. After the filing of the May 2009 complaint, various other nearly identical complaints also seeking certification as class actions were filed in federal district courts in Atlanta, Georgia; Orlando, Florida; and Las Vegas, Nevada. All of the cases were consolidated before  a single federal district court judge in Atlanta. A Consolidated Amended Complaint was filed in the consolidated action on February 1, 2010, which broadened the allegations to add claims that Delta and AirTran conspired to reduce capacity on competitive routes and to raise prices in violation of Section 1 of the Sherman Act. In addition to treble damages for the amount of first baggage fees paid to  AirTran and to Delta, the Consolidated Amended Complaint seeks injunctive relief against a broad range of alleged anticompetitive activities, as well as attorneys’ fees. On August 2, 2010, the Court dismissed plaintiffs’ claims that AirTran and Delta had violated Section 2 of the Sherman Act; the Court let stand the claims of a conspiracy with respect to the imposition of a first bag fee and the airlines’ capacity and pricing decisions. On June 30, 2010, the plaintiffs filed a motion to certify a class, which AirTran and Delta have opposed. The parties have submitted briefs on class certification, and the parties have filed motions to exclude the class certification opinions of each other’s expert. The parties engaged in extensive discovery, and discovery has now closed. On June 18, 2012, the parties filed a Stipulation and Order that plaintiffs have abandoned their claim that AirTran and Delta conspired to reduce capacity. On August 31, 2012, AirTran and Delta moved for summary judgment on all of plaintiffs’ remaining claims, but discovery disputes between plaintiffs and Delta delayed further briefing on summary judgment. On August 5, 2015, the Court entered an order granting class certification, which was vacated on August 17, 2015, to permit further briefing on class certification and AirTran’s motion to exclude plaintiffs’ expert. Thereafter, the parties filed motions to exclude the opinions of the other parties’ experts. On January 8, 2016, the parties completed briefing on defendants’ motions for summary judgment, plaintiffs’ motion for class certification, and the motions to exclude the opinions  of experts, and those motions have been submitted to the Court for decision. AirTran denies all allegations of wrongdoing, including those in the Consolidated Amended Complaint, and intends to defend vigorously any and all such allegations. Also, on June 30, 2015, the U.S. Department of Justice (“DOJ”) issued a Civil Investigative Demand (“CID”) to the Company. The CID seeks information and documents about the Company’s capacity from January 2010 to the present including public statements and communications with third parties about capacity. In June 2015, the Company also received a letter from the Connecticut Attorney General requesting information about capacity; and on August 21, 2015, the Attorney General of the State of Ohio issued an investigative demand seeking information and documents about the Company’s capacity from December 2013 to the present. The Company is cooperating fully with the DOJ CID and these two state inquiries. Further, on July 1, 2015, a complaint was filed in the United States District Court for the Southern District of New York on behalf of putative classes of consumers alleging collusion among the Company, American Airlines, Delta Air Lines, and United Airlines to limit capacity and maintain higher fares in violation of Section 1 of the Sherman Act. Since then, a number of similar class action complaints have been filed in the United States District Courts for the Central District of California,  the Northern District of California, the District of Columbia, the Middle District of Florida, the Southern District of Florida, the Northern District of Georgia, the Northern District of Illinois, the Southern District of Indiana, the Eastern District of Louisiana, the District of Minnesota, the District of New Jersey, the Eastern District of New York, the Southern District of New York, the Middle District of North Carolina, the District of Oklahoma, the Eastern District of Pennsylvania, the Northern District of Texas, the District of Vermont, and the Eastern District of Wisconsin. The complaints seek treble damages for periods that vary among the complaints, costs, attorneys’ fees, and injunctive relief. On October 13, 2015, the Judicial Panel on  Multi-District  Litigation  centralized  the  cases  to  the  United States District Court in the District of Columbia. The Court has not yet entered a scheduling order establishing a date for defendants to respond to the complaints. The Company intends to vigorously defend these civil cases. In addition, on July 8, 2015, the Company was named as a defendant in putative class action filed in British Columbia, Canada alleging that the Company, Air Canada, American Airlines, Delta Air  Lines and United Airlines colluded to restrict capacity and maintain higher fares for Canadian citizens traveling in the United States and for travel between the United States and Canada. Similar lawsuits were filed in Ontario, Quebec and Saskatchewan. The time for the Company to respond to the complaints has not yet expired. The Company intends to vigorously defend these civil cases in Canada. The Company is from time to time subject to various legal proceedings and claims arising in the ordinary course of business, including, but not limited to, examinations by the Internal Revenue Service. The Company’s management does not expect that the outcome in any of its currently ongoing legal proceedings or the outcome of any proposed adjustments presented to date by the Internal Revenue Service, individually or collectively, will have a material adverse effect on the Company’s financial condition, results of operations, or cash flow.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example text\n",
    "text = df.section_text[2461]\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy\n",
    "\n",
    "\"SpaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python.\n",
    "\n",
    "If you're working with a lot of text, you'll eventually want to know more about it. For example, what's it about? What do the words mean in context? Who is doing what to whom? What companies and products are mentioned? Which texts are similar to each other?\n",
    "\n",
    "SpaCy is designed specifically for production use and helps you build applications that process and \"understand\" large volumes of text. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning.\n",
    "\n",
    "SpaCy is not research software. It's built on the latest research, but it's designed to get things done. This leads to fairly different design decisions than NLTK or CoreNLP, which were created as platforms for teaching and research. The main difference is that SpaCy is integrated and opinionated. SpaCy tries to avoid asking the user to choose between multiple algorithms that deliver equivalent functionality. Keeping the menu small lets SpaCy deliver generally better performance and developer experience.\"\n",
    "\n",
    "#### Installation:\n",
    "- Windows: Download Microsoft Visual C++: \n",
    "1.\tGo to: https://www.visualstudio.com/downloads/#build-tools-for-visual-studio-2017\n",
    "2.\tDownload the first link for Visual Studio Community 2017\n",
    "3.\tDuring the install select the option to install Desktop with Development C++ (see image below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Desktop with Development C++\n",
    "Image(\"../../raw_data/images/visual_studio_community.png\", width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SpaCy Installation\n",
    "Run the following using git bash as an administrator (i.e. right click on the git bash logo and select 'Run as Admin')\n",
    "- conda install -c conda-forge spacy\n",
    "- python -m spacy download en\n",
    "\n",
    "##### if you run into an error try the following:\n",
    "- python -m spacy link en_core_web_sm en\n",
    "- SOURCE: https://github.com/explosion/spaCy/issues/950\n",
    "\n",
    "##### Optional to install a convolutional neural network model:\n",
    "- python -m spacy download en_core_web_lg\n",
    "\n",
    "##### Test the following code from git bash (even if previous step failed):\n",
    "start python\n",
    "- python -i\n",
    "\n",
    "test if SpaCy was downloaded\n",
    "- import spacy\n",
    "\n",
    "approach 1: test if model downloaded\n",
    "- nlp = spacy.load('en') \n",
    "\n",
    "appraoch 2: test this if spacy.load('en') failed\n",
    "- import en_core_web_sm\n",
    "- nlp = en_core_web_sm.load()\n",
    "\n",
    "Optional to install a convolutional neural network model (~800MB). This is the model I will use in class\n",
    "- python -m spacy download en_core_web_lg\n",
    "\n",
    "exit Python\n",
    "- exit()\n",
    "\n",
    "\n",
    "##### Optional - install on an AWS EC2 instance\n",
    "Instance: Amazon Linux 2 LTS Candidate 2 AMI (HVM), SSD Volume Type\n",
    "#!/bin/bash\n",
    "\n",
    "sudo yum update -y\n",
    "\n",
    "sudo yum groupinstall 'Development Tools' -y\n",
    "\n",
    "sudo easy_install pip\n",
    "\n",
    "sudo yum install python-devel -y\n",
    "\n",
    "sudo pip install spacy\n",
    "\n",
    "sudo python -m spacy download en_core_web_lg\n",
    "\n",
    "### SpaCy Features \n",
    "\n",
    "NAME |\tDESCRIPTION |\n",
    ":----- |:------|\n",
    "Tokenization|Segmenting text into words, punctuations marks etc.|\n",
    "Part-of-speech (POS) Tagging|Assigning word types to tokens, like verb or noun.|\n",
    "Dependency Parsing|\tAssigning syntactic dependency labels, describing the relations between individual tokens, like subject or object.|\n",
    "Lemmatization|\tAssigning the base forms of words. For example, the lemma of \"was\" is \"be\", and the lemma of \"rats\" is \"rat\".|\n",
    "Sentence Boundary Detection (SBD)|\tFinding and segmenting individual sentences.|\n",
    "Named Entity Recognition (NER)|\tLabelling named \"real-world\" objects, like persons, companies or locations.|\n",
    "Similarity|\tComparing words, text spans and documents and how similar they are to each other.|\n",
    "Text Classification|\tAssigning categories or labels to a whole document, or parts of a document.|\n",
    "Rule-based Matching|\tFinding sequences of tokens based on their texts and linguistic annotations, similar to regular expressions.|\n",
    "Training|\tUpdating and improving a statistical model's predictions.|\n",
    "Serialization|\tSaving objects to files or byte strings.|\n",
    "\n",
    "SOURCE: https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\alsherman\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\guild\\\\python.exe'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm which conda environment you are using - make sure it is one with SpaCy installed\n",
    "import sys\n",
    "sys.executable\n",
    "\n",
    "# if you have difficulty importing spacy try the following in git bash\n",
    "# conda install ipykernel --name Python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# read in a simple (small) English language model\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# another approach:\n",
    "# import en_core_web_sm\n",
    "# nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# read in a (large) convolutional neural network model\n",
    "# this will only work after the CNN model is downloaded (~800MB)\n",
    "# e.g. python -m spacy download en_core_web_lg\n",
    "nlp = spacy.load('en_core_web_lg') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f3e1b0971140>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# instantiate the document text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "# instantiate the document text\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the text from the SpaCy object\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_', '__bytes__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '_py_tokens', '_realloc', '_vector', '_vector_norm', 'cats', 'char_span', 'count_by', 'doc', 'ents', 'extend_tensor', 'from_array', 'from_bytes', 'from_disk', 'get_extension', 'get_lca_matrix', 'has_extension', 'has_vector', 'is_parsed', 'is_tagged', 'mem', 'merge', 'noun_chunks', 'noun_chunks_iterator', 'print_tree', 'sentiment', 'sents', 'set_extension', 'similarity', 'tensor', 'text', 'text_with_ws', 'to_array', 'to_bytes', 'to_disk', 'user_data', 'user_hooks', 'user_span_hooks', 'user_token_hooks', 'vector', 'vector_norm', 'vocab']\n"
     ]
    }
   ],
   "source": [
    "# which the SpaCy document methods and attributes\n",
    "print(dir(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Pipeline\n",
    "\n",
    "When you read the text into spaCy, e.g. doc = nlp(text), you are applying a pipeline of nlp processes to the text.\n",
    "by default spaCy applies a tagger, parser, and ner, but you can choose to add, replace, or remove these steps.\n",
    "Note: Removing unnecessary steps for a given nlp can lead to substantial descreses in processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=https://spacy.io/assets/img/pipeline.svg width=1000 height=200></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SpaCy pipeline\n",
    "spacy_url = 'https://spacy.io/assets/img/pipeline.svg'\n",
    "iframe = '<iframe src={} width=1000 height=200></iframe>'.format(spacy_url)\n",
    "HTML(iframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "SpaCy first tokenizes the text, i.e. segments it into words, punctuation and so on. This is done by applying rules specific to each language. For example, punctuation at the end of a sentence should be split off – whereas \"U.K.\" should remain one token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=https://spacy.io/assets/img/tokenization.svg width=650 height=400></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenization_url = 'https://spacy.io/assets/img/tokenization.svg'\n",
    "iframe = '<iframe src={} width=650 height=400></iframe>'.format(tokenization_url)\n",
    "HTML(iframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speech (POS) Tagging\n",
    "\n",
    "After tokenization, spaCy can parse and tag a given Doc. This is where the statistical model comes in, which enables spaCy to make a prediction of which tag or label most likely applies in this context. A model consists of binary data and is produced by showing a system enough examples for it to make predictions that generalize across the language – for example, a word following \"the\" in English is most likely a noun.\n",
    "\n",
    "Annotation | Description\n",
    ":----- |:------|\n",
    "Text |The original word text|\n",
    "Lemma |The base form of the word.|\n",
    "POS |The simple part-of-speech tag.|\n",
    "Tag |The detailed part-of-speech tag.|\n",
    "Dep |Syntactic dependency, i.e. the relation between tokens.|\n",
    "Shape |The word shape – capitalisation, punctuation, digits.|\n",
    "Is Alpha |Is the token an alpha character?|\n",
    "Is Stop |Is the token part of a stop list, i.e. the most common words of the language?|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a list of stop words from SpaCy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "print('Example stop words: {}'.format(list(STOP_WORDS)[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['that']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(nlp.vocab['that']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['that'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# search for word in the SpaCy vocabulary and\n",
    "# change the is_stop attribute to True (default is False)\n",
    "\n",
    "for word in STOP_WORDS:\n",
    "    nlp.vocab[word].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT            | LEMMA_          | POS_     | TAG_     | DEP_        | SHAPE_   | IS_ALPHA | IS_STOP  | \n",
      "A               | a               | DET      | DT       | det         | X        |        1 |        0 |\n",
      "complaint       | complaint       | NOUN     | NN       | ROOT        | xxxx     |        1 |        0 |\n",
      "alleging        | allege          | VERB     | VBG      | acl         | xxxx     |        1 |        0 |\n",
      "violations      | violation       | NOUN     | NNS      | dobj        | xxxx     |        1 |        0 |\n",
      "of              | of              | ADP      | IN       | prep        | xx       |        1 |        1 |\n",
      "federal         | federal         | ADJ      | JJ       | amod        | xxxx     |        1 |        0 |\n",
      "antitrust       | antitrust       | ADJ      | JJ       | amod        | xxxx     |        1 |        0 |\n",
      "laws            | law             | NOUN     | NNS      | pobj        | xxxx     |        1 |        0 |\n",
      "and             | and             | CCONJ    | CC       | cc          | xxx      |        1 |        1 |\n",
      "seeking         | seek            | VERB     | VBG      | conj        | xxxx     |        1 |        0 |\n",
      "certification   | certification   | NOUN     | NN       | dobj        | xxxx     |        1 |        0 |\n",
      "as              | as              | ADP      | IN       | mark        | xx       |        1 |        1 |\n",
      "a               | a               | DET      | DT       | det         | x        |        1 |        1 |\n",
      "class           | class           | NOUN     | NN       | compound    | xxxx     |        1 |        0 |\n",
      "action          | action          | NOUN     | NN       | nsubjpass   | xxxx     |        1 |        0 |\n",
      "was             | be              | VERB     | VBD      | auxpass     | xxx      |        1 |        1 |\n",
      "filed           | file            | VERB     | VBN      | advcl       | xxxx     |        1 |        0 |\n",
      "against         | against         | ADP      | IN       | prep        | xxxx     |        1 |        1 |\n",
      "Delta           | delta           | PROPN    | NNP      | compound    | Xxxxx    |        1 |        0 |\n",
      "Air             | air             | PROPN    | NNP      | compound    | Xxx      |        1 |        0 |\n"
     ]
    }
   ],
   "source": [
    "# print column headers\n",
    "print('{:15} | {:15} | {:8} | {:8} | {:11} | {:8} | {:8} | {:8} | '.format(\n",
    "    'TEXT','LEMMA_','POS_','TAG_','DEP_','SHAPE_','IS_ALPHA','IS_STOP'))\n",
    "\n",
    "# print various SpaCy POS attributes\n",
    "for token in doc[0:20]:\n",
    "    print('{:15} | {:15} | {:8} | {:8} | {:11} | {:8} | {:8} | {:8} |'.format(\n",
    "          token.text, token.lemma_, token.pos_, token.tag_, token.dep_\n",
    "        , token.shape_, token.is_alpha, token.is_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Dependency Parsing\n",
    "\n",
    "spaCy features a fast and accurate syntactic dependency parser, and has a rich API for navigating the tree. The parser also powers the sentence boundary detection, and lets you iterate over base noun phrases, or \"chunks\". You can check whether a Doc  object has been parsed with the doc.is_parsed attribute, which returns a boolean value. If this attribute is False, the default sentence iterator will raise an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check is document has been parsed (dependency parsing)\n",
    "doc.is_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT            | DEP        | HEAD TEXT  | HEAD POS   | CHILDREN                            | LEFTS                    \n",
      "A               | det        | complaint  | NOUN       | []                                  | []                       \n",
      "complaint       | ROOT       | complaint  | NOUN       | [A, alleging, .]                    | ['A']                    \n",
      "alleging        | acl        | complaint  | NOUN       | [violations, and, seeking]          | []                       \n",
      "violations      | dobj       | alleging   | VERB       | [of]                                | []                       \n",
      "of              | prep       | violations | NOUN       | [laws]                              | []                       \n",
      "federal         | amod       | laws       | NOUN       | []                                  | []                       \n",
      "antitrust       | amod       | laws       | NOUN       | []                                  | []                       \n",
      "laws            | pobj       | of         | ADP        | [federal, antitrust]                | ['federal', 'antitrust'] \n",
      "and             | cc         | alleging   | VERB       | []                                  | []                       \n",
      "seeking         | conj       | alleging   | VERB       | [certification, filed]              | []                       \n",
      "certification   | dobj       | seeking    | VERB       | []                                  | []                       \n",
      "as              | mark       | filed      | VERB       | []                                  | []                       \n",
      "a               | det        | action     | NOUN       | []                                  | []                       \n",
      "class           | compound   | action     | NOUN       | []                                  | []                       \n",
      "action          | nsubjpass  | filed      | VERB       | [a, class]                          | ['a', 'class']           \n",
      "was             | auxpass    | filed      | VERB       | []                                  | []                       \n",
      "filed           | advcl      | seeking    | VERB       | [as, action, was, against, in, on]  | ['as', 'action', 'was']  \n",
      "against         | prep       | filed      | VERB       | [Lines]                             | []                       \n",
      "Delta           | compound   | Lines      | PROPN      | []                                  | []                       \n",
      "Air             | compound   | Lines      | PROPN      | []                                  | []                       \n"
     ]
    }
   ],
   "source": [
    "print('{:15} | {:10} | {:10} | {:10} | {:35} | {:25}'.format(\n",
    "    'TEXT','DEP','HEAD TEXT','HEAD POS','CHILDREN','LEFTS'))\n",
    "\n",
    "for token in doc[0:20]:\n",
    "    print('{:15} | {:10} | {:10} | {:10} | {:35} | {:25}'.format(\n",
    "        token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "        str([child for child in token.children]),str([t.text for t in token.lefts])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOUN CHUNCKS:\n",
    "\n",
    "| **TERM** | Definition |\n",
    "|:---|:---:|\n",
    "| **Text** | The original noun chunk text |\n",
    "| **Root text** | The original text of the word connecting the noun chunk to the rest of the parse |\n",
    "| **Root dependency** | Dependency relation connecting the root to its head |\n",
    "| **Root head text** | The text of the root token's head |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_TEXT       | ROOT       | DEPENDENCY | TEXT                                    \n",
      "complaint       | ROOT       | complaint  | A complaint                             \n",
      "violations      | dobj       | alleging   | violations                              \n",
      "laws            | pobj       | of         | federal antitrust laws                  \n",
      "certification   | dobj       | seeking    | certification                           \n",
      "action          | nsubjpass  | filed      | a class action                          \n",
      "Lines           | pobj       | against    | Delta Air Lines                         \n",
      "Inc.            | conj       | Lines      | Inc.                                    \n",
      "AirTran         | conj       | Inc.       | AirTran                                 \n",
      "Court           | pobj       | in         | the United States District Court        \n",
      "District        | pobj       | for        | the Northern District                   \n",
      "Georgia         | pobj       | of         | Georgia                                 \n",
      "Atlanta         | pobj       | in         | Atlanta                                 \n",
      "May             | pobj       | on         | May                                     \n",
      "complaint       | nsubj      | alleged    | The complaint                           \n",
      "things          | pobj       | among      | other things                            \n",
      "AirTran         | nsubj      | attempted  | AirTran                                 \n",
      "travel          | dobj       | monopolize | air travel                              \n",
      "violation       | pobj       | in         | violation                               \n",
      "Section         | pobj       | of         | Section                                 \n",
      "Act             | pobj       | of         | the Sherman Act                         \n"
     ]
    }
   ],
   "source": [
    "print('{:15} | {:10} | {:10} | {:40}'.format('ROOT_TEXT','ROOT','DEPENDENCY','TEXT'))\n",
    "\n",
    "for chunk in list(doc.noun_chunks)[0:20]:\n",
    "    print('{:15} | {:10} | {:10} | {:40}'.format(\n",
    "        chunk.root.text, chunk.root.dep_, chunk.root.head.text, chunk.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dependency visualization\n",
    "# after you run this code, open another browser and go to http://localhost:5000\n",
    "# when you are done (before you run the next cell in the notebook) stop this cell\n",
    "displacy.serve(docs=doc, style='dep')\n",
    "\n",
    "# Another option: show visualization in Jupyter Notebook\n",
    "#displacy.render(docs=doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER)\n",
    "\n",
    "A named entity is a \"real-world object\" that's assigned a name – for example, a person, a country, a product, or a book title. spaCy can recognise various types of named entities in a document, by asking the model for a prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL      | ENTITY         \n",
      "ORG        | Delta Air Lines                                   \n",
      "ORG        | AirTran                                           \n",
      "GPE        | the United States District Court                  \n",
      "LOC        | the Northern District                             \n",
      "GPE        | Georgia                                           \n",
      "GPE        | Atlanta                                           \n",
      "DATE       | May 22, 2009                                      \n",
      "ORG        | AirTran                                           \n",
      "LAW        | Section 2                                         \n",
      "LAW        | the Sherman Act                                   \n",
      "ORG        | Delta                                             \n",
      "ORDINAL    | first                                             \n",
      "LAW        | Section 1                                         \n",
      "LAW        | the Sherman Act                                   \n",
      "GPE        | the United States                                 \n",
      "ORG        | Delta                                             \n",
      "ORG        | AirTran                                           \n",
      "DATE       | December 5, 2008                                  \n",
      "DATE       | May 2009                                          \n",
      "GPE        | Atlanta                                           \n"
     ]
    }
   ],
   "source": [
    "print('{:10} | {:15}'.format('LABEL','ENTITY'))\n",
    "\n",
    "for ent in doc.ents[0:20]:\n",
    "    print('{:10} | {:50}'.format(ent.label_, ent.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '_recalculate_indices', '_vector', '_vector_norm', 'as_doc', 'doc', 'end', 'end_char', 'ent_id', 'ent_id_', 'get_extension', 'get_lca_matrix', 'has_extension', 'has_vector', 'label', 'label_', 'lefts', 'lemma_', 'lower_', 'merge', 'n_lefts', 'n_rights', 'noun_chunks', 'orth_', 'rights', 'root', 'sent', 'sentiment', 'set_extension', 'similarity', 'start', 'start_char', 'string', 'subtree', 'text', 'text_with_ws', 'to_array', 'upper_', 'vector', 'vector_norm']\n"
     ]
    }
   ],
   "source": [
    "# ent methods and attributes\n",
    "print(dir(ent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Serving on port 5000...\n",
      "    Using the 'ent' visualizer\n",
      "\n",
      "\n",
      "    Shutting down server on port 5000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# entity visualization\n",
    "# after you run this code, open another browser and go to http://localhost:5000\n",
    "# when you are done (before you run the next cell in the notebook) stop this cell \n",
    "\n",
    "displacy.serve(doc, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "1. print all the distinct entities tagged with 'LAW'\n",
    "2. print all the distinct entities tagged with 'ORG'\n",
    "3. print all the distinct entities tagged with 'GPE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print all the distinct entities tagged as a law\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print all the distinct entities tagged as an organization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print all the distinct entities tagged as a geopolitical entity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Collections - DefaultDict\n",
    "\n",
    "Usually, a Python dictionary throws a KeyError if you try to get an item with a key that is not currently in the dictionary. The defaultdict in contrast will simply create any items that you try to access (provided of course they do not exist yet). To create such a \"default\" item, it calls the function object that you pass in the constructor (more precisely, it's an arbitrary \"callable\" object, which includes function and type objects). For the first example, default items are created using int(), which will return the integer object 0. For the second example, default items are created using list(), which returns a new empty list object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = ['The','airline','baggage','fees','and','food','fees','are','outrageous']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'The'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-1bd25484247e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m  \u001b[1;31m# cannot add if the key does not exist\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'The'"
     ]
    }
   ],
   "source": [
    "# WRONG APPROACH - KeyError!\n",
    "\n",
    "# try to create a word count dict with new keys\n",
    "d = {}\n",
    "for word in sentence:\n",
    "    d[word] += 1  # cannot add if the key does not exist\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'baggage': 1, 'are': 1, 'and': 1, 'fees': 2, 'food': 1, 'outrageous': 1, 'The': 1, 'airline': 1})\n"
     ]
    }
   ],
   "source": [
    "d = defaultdict(int)  # define the type of data the dict stores\n",
    "for word in sentence:\n",
    "    d[word] += 1  # can add to unassigned keys\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Collections - Counter\n",
    "\n",
    "A Counter is a dict subclass for counting hashable objects. It is an unordered collection where elements are stored as dictionary keys and their counts are stored as dictionary values. Counts are allowed to be any integer value including zero or negative counts. The Counter class is similar to bags or multisets in other languages.\n",
    "\n",
    "SOURCE: https://docs.python.org/2/library/collections.html#collections.Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Canada': 3, 'the United States': 3, 'Atlanta': 3, 'Florida': 3, 'New York': 3, 'Georgia': 3, 'the United States District Court': 2, 'the District of Columbia': 2, 'California': 2, 'the District of Oklahoma': 1, 'Louisiana': 1, 'the District of Minnesota': 1, 'Connecticut': 1, 'Quebec': 1, 'Multi-District  ': 1, 'the District of Vermont': 1, 'the District of New Jersey': 1, 'Ontario': 1, 'Las Vegas': 1, 'Pennsylvania': 1, 'Indiana': 1, 'Wisconsin': 1, 'British Columbia': 1, 'Saskatchewan': 1, 'Nevada': 1, 'Texas': 1, 'the United States District Courts': 1, 'Orlando': 1, 'Illinois': 1, 'North Carolina': 1})\n"
     ]
    }
   ],
   "source": [
    "# count the number of times each GPE appears\n",
    "print(Counter(ent.text for ent in doc.ents if 'GPE' in ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iterrtools - combinations\n",
    "\n",
    "\"The itertools module standardizes a core set of fast, memory efficient tools that are useful by themselves or in combination. Together, they form an “iterator algebra” making it possible to construct specialized tools succinctly and efficiently in pure Python.\n",
    "\n",
    "**Combinations**\n",
    "- Return r length subsequences of elements from the input iterable.\n",
    "- Combinations are emitted in lexicographic sort order. So, if the input iterable is sorted, the combination tuples will be produced in sorted order.\n",
    "- Elements are treated as unique based on their position, not on their value. So if the input elements are unique, there will be no repeat values in each combination.\n",
    "\n",
    "SOURCE: https://docs.python.org/3.4/library/itertools.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "airlines = ['Southwest','American','Delta','United']\n",
    "for combo in combinations(airlines, 2):\n",
    "    print(combo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sorted\n",
    "\n",
    "sorted(iterable, key=None, reverse=False)\n",
    "\n",
    "- Return a new sorted list from the items in iterable.\n",
    "- Has two optional arguments which must be specified as keyword arguments.\n",
    "- key specifies a function of one argument that is used to extract a comparison key from each list element: key=str.lower. The default value is None (compare the elements directly).\n",
    "- reverse is a boolean value. If set to True, then the list elements are sorted as if each comparison were reversed.\n",
    "\n",
    "SOURCE: https://docs.python.org/3/library/functions.html#sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "airlines =[('airlines2',3),('airlines3',2),('airlines1',1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('airlines1', 1), ('airlines2', 3), ('airlines3', 2)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(airlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('airlines1', 1), ('airlines3', 2), ('airlines2', 3)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(airlines, key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('airlines2', 3), ('airlines3', 2), ('airlines1', 1)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(airlines, key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('airlines1', 3), ('airlines2', 2), ('airlines3', 1)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort based on the last character of the first term\n",
    "sorted(airlines, key=lambda x:x[0][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "1. Count how many time each individual entity appears\n",
    "2. Create a mapping that keeps track of every combination of entities pairs that appear in the same sentence\n",
    "3. Count how many times each entity combo appears\n",
    "4. Print the entity combos (using sorted) in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# default dict to store all combinations of airlines that appear together \n",
    "entity_relations = \n",
    "\n",
    "# list to sort and count how often each entity appears \n",
    "counter_entities = \n",
    "\n",
    "for sent in doc.sents:\n",
    "    # extract entities for each sentence\n",
    "    sent = \n",
    "\n",
    "    # store all entities tagged as an organization\n",
    "    entities = \n",
    "\n",
    "    # add the entities from the current sentence to counter_entities\n",
    "    \n",
    "    \n",
    "    # create combinations and increment the count in entity_relations each time combo appears\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'AirTran': 11, 'Delta': 9, 'Company': 7, 'Court': 5, 'CID': 3, 'United Airlines': 2, 'American Airlines': 2, 'Delta Air Lines': 2, 'the Internal Revenue Service': 2, 'the U.S. Department of Justice': 1, 'United States District Court': 1, 'DOJ': 1, 'Judicial Panel': 1, 'the Consolidated Amended Complaint': 1, 'the State of Ohio': 1, 'Consolidated Amended Complaint': 1, 'Company’s': 1, 'the Company, Air Canada': 1, 'a Civil Investigative Demand': 1, 'Delta Air  Lines': 1, 'plaintiffs’': 1})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(counter_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('AirTran', 'Delta'), 8),\n",
       " (('United Airlines', 'Company'), 2),\n",
       " (('Court', 'AirTran'), 2),\n",
       " (('American Airlines', 'United Airlines'), 2),\n",
       " (('American Airlines', 'Company'), 2),\n",
       " (('AirTran', 'the Consolidated Amended Complaint'), 1),\n",
       " (('the Company, Air Canada', 'Company'), 1),\n",
       " (('the State of Ohio', 'Company'), 1),\n",
       " (('CID', 'Company'), 1),\n",
       " (('CID', 'DOJ'), 1),\n",
       " (('Delta Air Lines', 'Company'), 1),\n",
       " (('a Civil Investigative Demand', 'the U.S. Department of Justice'), 1),\n",
       " (('Company’s', 'the Internal Revenue Service'), 1),\n",
       " (('the Company, Air Canada', 'Delta Air  Lines'), 1),\n",
       " (('CID', 'a Civil Investigative Demand'), 1),\n",
       " (('AirTran', 'Consolidated Amended Complaint'), 1),\n",
       " (('United Airlines', 'Delta Air Lines'), 1),\n",
       " (('CID', 'the U.S. Department of Justice'), 1),\n",
       " (('Company’s', 'Company'), 1),\n",
       " (('Court', 'Delta'), 1),\n",
       " (('AirTran', 'plaintiffs’'), 1),\n",
       " (('Delta', 'plaintiffs’'), 1),\n",
       " (('Company', 'the Internal Revenue Service'), 1),\n",
       " (('United Airlines', 'Delta Air  Lines'), 1),\n",
       " (('the Company, Air Canada', 'American Airlines'), 1),\n",
       " (('Delta Air  Lines', 'Company'), 1),\n",
       " (('AirTran', 'Delta Air Lines'), 1),\n",
       " (('Delta', 'Consolidated Amended Complaint'), 1),\n",
       " (('American Airlines', 'Delta Air  Lines'), 1),\n",
       " (('the Company, Air Canada', 'United Airlines'), 1),\n",
       " (('American Airlines', 'Delta Air Lines'), 1)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the entity pairs in descending order\n",
    "sorted(entity_relations.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract entity pairs from all sections with the text Southwest Airlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-6155c0a03f08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msection_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Southwest Airlines'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msection_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "texts = df[df.section_text.str.contains('Southwest Airlines')].section_text\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "all_ents = []\n",
    "\n",
    "for doc in nlp.pipe(texts, batch_size=1000, disable=['tagger','ner']):\n",
    "    # split the document into sentences\n",
    "    sentences = [sentence.text for sentence in doc.sents]\n",
    "    for sent in nlp.pipe(sentences, batch_size=10, disable=['parser','tagger']):\n",
    "        # store all entities tagged as an organization\n",
    "        entities = list(set(ent.text for ent in sent.ents if 'ORG' in ent.label_))\n",
    "        # skip sentence that do not have at least 2 entities to connect \n",
    "        if len(entities) < 2:\n",
    "            continue\n",
    "        # store all entities to use later to filter relevant entities\n",
    "        for e in entities:\n",
    "            all_ents.append(e)\n",
    "        # create mapping with all combonitions of entity pairs in the sentence\n",
    "        for combo in combinations(entities, 2):\n",
    "            entity_relations[combo] += 1\n",
    "    \n",
    "set(all_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(all_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " airlines_map = {\n",
    " '  Southwest':'Southwest',\n",
    " '  Southwest (':'Southwest',\n",
    " '  United   Airlines/':'United',\n",
    " 'ATA  Airlines':'ATA Airlines',\n",
    " 'ATA Airlines':'ATA Airlines',\n",
    " 'AirTran':'AirTran',\n",
    " 'AirTran Airways':'AirTran',\n",
    " 'AirTran Airways, Inc.':'AirTran',\n",
    " 'America West Airlines':'America West Airlines',\n",
    " 'American Airlines':'American',\n",
    " 'American Airlines, Inc.':'American',\n",
    " 'American Eagle Airlines':'American Eagle Airlines',\n",
    " 'Delta':'Delta',\n",
    " 'Delta Air Lines':'Delta',\n",
    " 'Global Airlines':'Global Airlines',\n",
    " 'JetBlue':'JetBlue',\n",
    " 'Northwest Airlines':'Northwest',\n",
    " 'Northwest Airlines Corporation':'Northwest',\n",
    " 'Northwest Airlines/ Continental Airlines':'Northwest',\n",
    " 'People Southwest Airlines':'People Southwest Airlines',\n",
    " 'People of Southwest Airlines':'People Southwest Airlines',\n",
    " 'Southwest':'Southwest',\n",
    " 'Southwest  Airlines':'Southwest',\n",
    " 'Southwest  Airlines  Co.':'Southwest',\n",
    " 'Southwest Airlines':'Southwest',\n",
    " 'Southwest Airlines Air Travel':'Southwest',\n",
    " 'Southwest Airlines Co.':'Southwest',\n",
    " 'Southwest Airlines, Co.':'Southwest',\n",
    " 'US Airways':'US Airways',\n",
    " 'US Airways Group':'US Airways',\n",
    " 'US Airways Group, Inc.':'US Airways',\n",
    " 'USAirways':'US Airways',\n",
    " 'United':'United',\n",
    " 'United Airlines':'United',\n",
    " 'the Southwest Airlines  ':'Southwest',\n",
    " 'the Southwest Airlines Co.':'Southwest'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_pairs = []\n",
    "for items, count in entity_relations.items():\n",
    "    if (items[0] in airlines_map) and (items[1] in airlines_map):\n",
    "        airline1 = airlines_map[items[0]]\n",
    "        airline2 = airlines_map[items[1]]\n",
    "        airline_pairs.append((airline1, airline2, count))\n",
    "\n",
    "airline_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'entity_relations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-40daf61d3b7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentity_relations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mentity1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mentity2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'entity_relations' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for entities, value in entity_relations.items():\n",
    "    entity1 = entities[0]\n",
    "    entity2 = entities[1]\n",
    "    \n",
    "    G.add_edge(entity1, entity2, weight=value)\n",
    "\n",
    "# positions for all nodes\n",
    "pos = nx.spring_layout(G, k=5)\n",
    "\n",
    "# nodes\n",
    "nx.draw_networkx_nodes(G, pos, node_size=10)\n",
    "\n",
    "# edges\n",
    "for (u, v, d) in G.edges(data=True):\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=[(u,v)], width=d['weight'], alpha=.2)\n",
    "\n",
    "# labels\n",
    "nx.draw_networkx_labels(G, pos, font_size=10,  font_family='sans-serif')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Identify Relevant Text (Rule-based Matching)\n",
    "\n",
    "Finding sequences of tokens based on their texts and linguistic annotations, similar to regular expressions. We will use this to filter and extract relevant text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=https://spacy.io/usage/linguistic-features#rule-based-matching width=1000 height=700></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rule_basesd_matching_url = 'https://spacy.io/usage/linguistic-features#rule-based-matching'\n",
    "iframe = '<iframe src={} width=1000 height=700></iframe>'.format(rule_basesd_matching_url)\n",
    "HTML(iframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The Matcher identifies text from rules we specify\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a function to specify what to do with the matching text\n",
    "\n",
    "def collect_sents(matcher, doc, i, matches):\n",
    "    \"\"\"  collect and transform matching text\n",
    "\n",
    "    :param matcher: Matcher object\n",
    "    :param doc: is the full document to search for text patterns\n",
    "    :param i: is the index of the text matches\n",
    "    :param matches: matches found in the text\n",
    "    \"\"\"\n",
    "    \n",
    "    match_id, start, end = matches[i]  # indices of matched term\n",
    "    span = doc[start:end]              # extract matched term\n",
    "    \n",
    "    print('span: {} | start_ind:{:5} | end_ind:{:5} | id:{}'.format(\n",
    "        span, start, end, match_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "span: fees | start_ind:   80 | end_ind:   81 | id:7125196598045271428\n",
      "span: fees | start_ind:  125 | end_ind:  126 | id:7125196598045271428\n",
      "span: fees | start_ind:  252 | end_ind:  253 | id:7125196598045271428\n",
      "span: fees | start_ind:  281 | end_ind:  282 | id:7125196598045271428\n",
      "span: fees | start_ind:  933 | end_ind:  934 | id:7125196598045271428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(7125196598045271428, 80, 81),\n",
       " (7125196598045271428, 125, 126),\n",
       " (7125196598045271428, 252, 253),\n",
       " (7125196598045271428, 281, 282),\n",
       " (7125196598045271428, 933, 934)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set a pattern of text to collect\n",
    "# find all mentions of the word fees \n",
    "pattern = [{'LOWER':'fees'}] # LOWER coverts words to lowercase before matching\n",
    "\n",
    "# instantiate matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# add pattern to the matcher (one matcher can look for many unique patterns)\n",
    "# provice a pattern name, function to apply to matches, pattern to identify\n",
    "matcher.add('fee', collect_sents, pattern)\n",
    "\n",
    "# pass the doc to the matcher to run the collect_sents function\n",
    "matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPAN: bag fees\n",
      "SENT: The complaint alleged, among other things, that AirTran attempted to monopolize air travel in violation of Section 2 of the Sherman Act, and conspired with Delta in imposing $15-per-bag fees for the first item of checked luggage in violation of Section 1 of the Sherman Act.\n",
      "\n",
      "SPAN: baggage fees\n",
      "SENT: In addition to treble damages for the amount of first baggage fees paid to  AirTran and to Delta, the Consolidated Amended Complaint seeks injunctive relief against a broad range of alleged anticompetitive activities, as well as attorneys’ fees.\n",
      "\n",
      "SPAN: attorneys’ fees\n",
      "SENT: In addition to treble damages for the amount of first baggage fees paid to  AirTran and to Delta, the Consolidated Amended Complaint seeks injunctive relief against a broad range of alleged anticompetitive activities, as well as attorneys’ fees.\n",
      "\n",
      "SPAN: attorneys’ fees\n",
      "SENT: The complaints seek treble damages for periods that vary among the complaints, costs, attorneys’ fees, and injunctive relief.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(7125196598045271428, 79, 81),\n",
       " (7125196598045271428, 251, 253),\n",
       " (7125196598045271428, 280, 282),\n",
       " (7125196598045271428, 932, 934)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the function to print the sentence of the matched term (span)\n",
    "\n",
    "def collect_sents(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "    print('SPAN: {}'.format(span))\n",
    "\n",
    "    # span.sent provides the sentence that contains the span\n",
    "    print('SENT: {}'.format(span.sent))\n",
    "    print()\n",
    "\n",
    "\n",
    "# update the pattern to look for any noun preceeding the term 'fees'\n",
    "pattern = [{'POS': 'NOUN', 'OP': '+'},{'LOWER':'fees'}]\n",
    "matcher = Matcher(nlp.vocab)  # reinstantiate the matcher to remove previous patterns\n",
    "matcher.add('fee', collect_sents, pattern)\n",
    "matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7125196598045271428, 79, 81),\n",
       " (7125196598045271428, 251, 253),\n",
       " (7125196598045271428, 280, 282),\n",
       " (7125196598045271428, 932, 934)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the function to collect sentences\n",
    "\n",
    "def collect_sents(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "        \n",
    "    # update matched data collections\n",
    "    matched_sents.append(span.sent)\n",
    "\n",
    "\n",
    "matched_sents = []  # container for sentences\n",
    "pattern = [{'POS': 'NOUN', 'OP': '+'},{'LOWER':'fees'}]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('fee', collect_sents, pattern)\n",
    "matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{The complaint alleged, among other things, that AirTran attempted to monopolize air travel in violation of Section 2 of the Sherman Act, and conspired with Delta in imposing $15-per-bag fees for the first item of checked luggage in violation of Section 1 of the Sherman Act.,\n",
       " In addition to treble damages for the amount of first baggage fees paid to  AirTran and to Delta, the Consolidated Amended Complaint seeks injunctive relief against a broad range of alleged anticompetitive activities, as well as attorneys’ fees.,\n",
       " The complaints seek treble damages for periods that vary among the complaints, costs, attorneys’ fees, and injunctive relief.}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review matches\n",
    "set(matched_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'fees': 1})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the function to count matches using defaultdict\n",
    "\n",
    "def collect_sents(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "    \n",
    "    # update matched data collections\n",
    "    ent_count[span.text] += 1  # defaultdict keys must use span.text not span!\n",
    "\n",
    "\n",
    "ent_count = defaultdict(int)\n",
    "pattern = [{'LOWER':'fees'}]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('fees', collect_sents, pattern)\n",
    "matcher(doc)\n",
    "\n",
    "ent_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'Landing fees': 1})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update the pattern to look for a noun describing the fee\n",
    "\n",
    "ent_count = defaultdict(int)\n",
    "pattern = [{'POS': 'NOUN', 'OP': '+'},{'LOWER':'fees'}]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('fees', collect_sents, pattern)\n",
    "matcher(doc)\n",
    "\n",
    "ent_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "If you have a sequence of documents to process, you should use the Language.pipe()  method. The method takes an iterator of texts, and accumulates an internal buffer, which it works on in parallel. It then yields the documents in order, one-by-one.\n",
    "\n",
    "- batch_size: number of docs to process per thread\n",
    "- n_threads: number threads to use (-1 is the default that let's SpaCy decide)\n",
    "- disable: Names of pipeline components to disable to speed up text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.pipeline import Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get multiple sections with the term fees\n",
    "# use SpaCy to determine what type of fees\n",
    "\n",
    "texts = df[df['section_text'].str.contains('fees')]['section_text'].values[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'Landing fees': 2})\n",
      "Wall time: 39.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ent_count = defaultdict(int) # reset defaultdict\n",
    "\n",
    "for doc in nlp.pipe(texts): # ['parser','tagger','ner']\n",
    "    matcher(doc) # match on your text\n",
    "\n",
    "print(ent_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy - Tips for faster processing\n",
    "\n",
    "You can substantially speed up the time it takes SpaCy to read a document by disabling components of the NLP that are not necessary for a given task.\n",
    "\n",
    "- Disable options: **parser, tagger, ner**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'Landing fees': 2})\n",
      "Wall time: 9.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# reset defaultdict\n",
    "ent_count = defaultdict(int)\n",
    "\n",
    "# disable the parser and ner, as we only use POS tagging in this example\n",
    "# processing occurs ~5x faster\n",
    "for doc in nlp.pipe(texts, batch_size=100, disable=['parser','ner']):  \n",
    "    matcher(doc) # match on your text\n",
    "\n",
    "print(ent_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {})\n",
      "Wall time: 659 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ent_count = defaultdict(int) # reset defaultdict\n",
    "\n",
    "# disable the parser and ner, as we only use POS tagging in this example\n",
    "# processing occurs ~75x faster, but doesn't work as the tagger is needed for the matcher\n",
    "for doc in nlp.pipe(texts, batch_size=100, disable=['parser','tagger','ner']):\n",
    "    matcher(doc) # match on your text\n",
    "\n",
    "print(ent_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the different risk types by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get multiple sections with the term fees\n",
    "texts = df[df['section_text'].str.contains('fees')][['filename','section_text']].values\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These fee types were extracted using the below code. \n",
    "# For the purpose of the lesson, instead of running it twice I have pulled them out\n",
    "# and grouped similar fees\n",
    "\n",
    "fee_types = {\n",
    "    'Landing fees':'landing'\n",
    "  , 'agriculture inspection fees':'agriculture'\n",
    "  , 'attorneys fees':'attorneys'\n",
    "  , 'attorneys’ fees':'attorneys'\n",
    "  , 'bag fees':'bag'\n",
    "  , 'baggage fees':'bag'\n",
    "  , 'card fees':'card'\n",
    "  , 'card interchange fees':'card'\n",
    "  , 'card processing fees':'card'\n",
    "  , 'change fees':'change'\n",
    "  , 'credit card fees':'card'\n",
    "  , 'credit card interchange fees':'card'\n",
    "  , 'credit card processing fees':'card'\n",
    "  , 'customs fees':'customs'\n",
    "  , 'enplanement fees':'enplanement'\n",
    "  , 'experts’ fees':'experts'\n",
    "  , 'inspection fees':'inspection'\n",
    "  , 'interchange fees':'interchange'\n",
    "  , 'l1nding fees':'landing'\n",
    "  , 'landing fees':'landing'\n",
    "  , 'passenger fees':'passenger'\n",
    "  , 'printing fees':'printing'\n",
    "  , 'processing fees':'processing'\n",
    "  , 'refund passenger fees':'refund'\n",
    "  , 'security fees':'security'\n",
    "  , 'service fees':'service'\n",
    "  , 'user fees':'user'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "return this if the key is not in the dict\n"
     ]
    }
   ],
   "source": [
    "# dict get\n",
    "# returns value if key is in dict, otherwise returns a value of your choice\n",
    "print(fee_types.get('user fees', 'return this if the key is not in the dict'))\n",
    "print(fee_types.get('not a value', 'return this if the key is not in the dict'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create simple matcher function and pattern\n",
    "\n",
    "def collect_sents(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "\n",
    "    # replace the fee type\n",
    "    fee = fee_types.get(span.text, span.text)\n",
    "    ent_count[fee] += 1\n",
    "    \n",
    "pattern = [{'POS': 'NOUN', 'OP': '+'},{'LOWER':'fees'}]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('risk', collect_sents, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "years = defaultdict(dict)\n",
    "for year, text in texts:\n",
    "    ent_count = defaultdict(int)               # reset ent_count for each year\n",
    "    doc = nlp(text, disable=['parser','ner'])  # disable unnessecary components\n",
    "    matcher(doc)                               # match on your text\n",
    "    \n",
    "    for key, val in ent_count.items():\n",
    "        years[year][key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1d56d2056a0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d5626f7fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# view the fees by year|\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(pd.DataFrame(years).T.fillna(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced SpaCy (Bonus Material)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Matching: Avoid Multiple Term Matches\n",
    "\n",
    "When using rule-based matching, SpaCy may match the same term multiple times if it is part of different n-term pairs with one term contained in another. For instance, 'integration services' in 'system integration services.'\n",
    "\n",
    "To avoid matching these terms multiple times, we can add to the collect_sents function to check if each term is contained in the previous term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_sents(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "    sent = span.sent\n",
    "    \n",
    "    # lemmatize the matched spans\n",
    "    entity = span.lemma_.lower()\n",
    "            \n",
    "    # explicity add the first entity without checking if it matches other terms\n",
    "    # as there is no previous span to check    \n",
    "    if i == 0:\n",
    "        ent_count[entity] += 1\n",
    "        ent_sents[entity].append(sent)\n",
    "        matched_sents.append(sent)\n",
    "        return\n",
    "\n",
    "    # get the span, entity, and sentence from the previous match\n",
    "    # if more than one match exist\n",
    "    last_match_id, last_start, last_end = matches[i-1]\n",
    "    last_span = doc[last_start : last_end]\n",
    "    last_entity = last_span.text.lower()\n",
    "    last_sent = last_span.sent\n",
    "\n",
    "    # to avoid adding duplicates when one term is contained in another \n",
    "    # (e.g. 'integration services' in 'system integration services')\n",
    "    # make sure new spans are unique\n",
    "    distinct_entity = (entity not in last_entity) or (sent != last_sent)\n",
    "    not_duplicate_entity = (entity != last_entity) or (sent != last_sent)\n",
    "    \n",
    "    # update collections for unique data\n",
    "    if distinct_entity and not_duplicate_entity:\n",
    "        ent_count[entity] += 1\n",
    "        ent_sents[entity].append(sent)\n",
    "        matched_sents.append(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multiple Patterns\n",
    "\n",
    "SpaCy matchers can use multiple patterns. Each pattern can be added to the Matcher individually with match.add and can use their own collect_sents function. Or use *patterns to add multiple patterns to the matcher at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matched_sents = []\n",
    "ent_sents  = defaultdict(list)\n",
    "ent_count = defaultdict(int)\n",
    "\n",
    "# multiple patterns\n",
    "pattern = [[{'POS': 'NOUN', 'OP': '+'},{'LOWER': 'fee'}]\n",
    "           , [{'POS': 'NOUN', 'OP': '+'},{'LOWER': 'fees'}]]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# *patterns to add multiple patterns with the same collect_sents function\n",
    "matcher.add('all_fees', collect_sents, *pattern)\n",
    "\n",
    "texts = df[df['section_text'].str.contains('fee')]['section_text'].values[0:5]\n",
    "for doc in nlp.pipe(texts, batch_size=100, disable=['ner']):\n",
    "    matches = matcher(doc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ent_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ent_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subject Verb Object (S,V,O) Extraction\n",
    "\n",
    "SOURCES: \n",
    "- http://textacy.readthedocs.io/en/latest/_modules/textacy/extract.html#subject_verb_object_triples\n",
    "- http://textacy.readthedocs.io/en/latest/_modules/textacy/spacy_utils.html#get_main_verbs_of_sent\n",
    "- https://github.com/chartbeat-labs/textacy/blob/master/textacy/constants.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import nanmin, nanmax, zeros, NaN\n",
    "from itertools import takewhile\n",
    "from spacy.parts_of_speech import CONJ, DET, NOUN, VERB\n",
    "from spacy.tokens.span import Span as SpacySpan\n",
    "\n",
    "NUMERIC_NE_TYPES = {'ORDINAL', 'CARDINAL', 'MONEY', 'QUANTITY', 'PERCENT', 'TIME', 'DATE'}\n",
    "SUBJ_DEPS = {'agent', 'csubj', 'csubjpass', 'expl', 'nsubj', 'nsubjpass'}\n",
    "OBJ_DEPS = {'attr', 'dobj', 'dative', 'oprd'}\n",
    "AUX_DEPS = {'aux', 'auxpass', 'neg'}\n",
    "\n",
    "def subject_verb_object_triples(doc):\n",
    "    \"\"\"\n",
    "    Extract an ordered sequence of subject-verb-object (SVO) triples from a\n",
    "    spacy-parsed doc. Note that this only works for SVO languages.\n",
    "\n",
    "    Args:\n",
    "        doc (``textacy.Doc`` or ``spacy.Doc`` or ``spacy.Span``)\n",
    "\n",
    "    Yields:\n",
    "        Tuple[``spacy.Span``, ``spacy.Span``, ``spacy.Span``]: the next 3-tuple\n",
    "            of spans from ``doc`` representing a (subject, verb, object) triple,\n",
    "            in order of appearance\n",
    "    \"\"\"\n",
    "    # TODO: What to do about questions, where it may be VSO instead of SVO?\n",
    "    # TODO: What about non-adjacent verb negations?\n",
    "    # TODO: What about object (noun) negations?\n",
    "    if isinstance(doc, SpacySpan):\n",
    "        sents = [doc]\n",
    "    else:  # textacy.Doc or spacy.Doc\n",
    "        sents = doc.sents\n",
    "\n",
    "    for sent in sents:\n",
    "        start_i = sent[0].i\n",
    "\n",
    "        verbs = get_main_verbs_of_sent(sent)\n",
    "        for verb in verbs:\n",
    "            subjs = get_subjects_of_verb(verb)\n",
    "            if not subjs:\n",
    "                continue\n",
    "            objs = get_objects_of_verb(verb)\n",
    "            if not objs:\n",
    "                continue\n",
    "\n",
    "            # add adjacent auxiliaries to verbs, for context\n",
    "            # and add compounds to compound nouns\n",
    "            verb_span = get_span_for_verb_auxiliaries(verb)\n",
    "            verb = sent[verb_span[0] - start_i: verb_span[1] - start_i + 1]\n",
    "            for subj in subjs:\n",
    "                subj = sent[get_span_for_compound_noun(subj)[0] - start_i: subj.i - start_i + 1]\n",
    "                for obj in objs:\n",
    "                    if obj.pos == NOUN:\n",
    "                        span = get_span_for_compound_noun(obj)\n",
    "                    elif obj.pos == VERB:\n",
    "                        span = get_span_for_verb_auxiliaries(obj)\n",
    "                    else:\n",
    "                        span = (obj.i, obj.i)\n",
    "                    obj = sent[span[0] - start_i: span[1] - start_i + 1]\n",
    "\n",
    "                    yield (subj, verb, obj)\n",
    "\n",
    "def get_main_verbs_of_sent(sent):\n",
    "    \"\"\"Return the main (non-auxiliary) verbs in a sentence.\"\"\"\n",
    "    return [tok for tok in sent\n",
    "            if tok.pos == VERB and tok.dep_ not in {'aux', 'auxpass'}]\n",
    "\n",
    "def get_subjects_of_verb(verb):\n",
    "    \"\"\"Return all subjects of a verb according to the dependency parse.\"\"\"\n",
    "    subjs = [tok for tok in verb.lefts\n",
    "             if tok.dep_ in SUBJ_DEPS]\n",
    "    # get additional conjunct subjects\n",
    "    subjs.extend(tok for subj in subjs for tok in _get_conjuncts(subj))\n",
    "    return subjs\n",
    "\n",
    "def get_objects_of_verb(verb):\n",
    "    \"\"\"\n",
    "    Return all objects of a verb according to the dependency parse,\n",
    "    including open clausal complements.\n",
    "    \"\"\"\n",
    "    objs = [tok for tok in verb.rights\n",
    "            if tok.dep_ in OBJ_DEPS]\n",
    "    # get open clausal complements (xcomp)\n",
    "    objs.extend(tok for tok in verb.rights\n",
    "                if tok.dep_ == 'xcomp')\n",
    "    # get additional conjunct objects\n",
    "    objs.extend(tok for obj in objs for tok in _get_conjuncts(obj))\n",
    "    return objs\n",
    "\n",
    "def _get_conjuncts(tok):\n",
    "    \"\"\"\n",
    "    Return conjunct dependents of the leftmost conjunct in a coordinated phrase,\n",
    "    e.g. \"Burton, [Dan], and [Josh] ...\".\n",
    "    \"\"\"\n",
    "    return [right for right in tok.rights\n",
    "            if right.dep_ == 'conj']\n",
    "\n",
    "def get_span_for_verb_auxiliaries(verb):\n",
    "    \"\"\"\n",
    "    Return document indexes spanning all (adjacent) tokens\n",
    "    around a verb that are auxiliary verbs or negations.\n",
    "    \"\"\"\n",
    "    min_i = verb.i - sum(1 for _ in takewhile(lambda x: x.dep_ in AUX_DEPS,\n",
    "                                              reversed(list(verb.lefts))))\n",
    "    max_i = verb.i + sum(1 for _ in takewhile(lambda x: x.dep_ in AUX_DEPS,\n",
    "                                              verb.rights))\n",
    "    return (min_i, max_i)\n",
    "\n",
    "def get_span_for_compound_noun(noun):\n",
    "    \"\"\"\n",
    "    Return document indexes spanning all (adjacent) tokens\n",
    "    in a compound noun.\n",
    "    \"\"\"\n",
    "    min_i = noun.i - sum(1 for _ in takewhile(lambda x: x.dep_ == 'compound',\n",
    "                                              reversed(list(noun.lefts))))\n",
    "    return (min_i, noun.i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples = [(s,v,o) for s,v,o in subject_verb_object_triples(doc)]\n",
    "triples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
