{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase (collocation) Detection Solution\n",
    "\n",
    "###### Author: Alex Sherman | alsherman@deloitte.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agenda\n",
    "1. Acronym replacement\n",
    "2. SpaCy POS phrases\n",
    "3. Gensim Phrases and Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from collections import defaultdict\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from IPython.core.display import display, HTML\n",
    "from configparser import ConfigParser, ExtendedInterpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration for data, acronyms, and gensim paths\n",
    "config = ConfigParser(interpolation=ExtendedInterpolation())\n",
    "config.read('../../config.ini')\n",
    "\n",
    "DB_PATH = config['DATABASES']['PROJECT_DB_PATH']\n",
    "AIRLINE_ACRONYMS_FILEPATH = config['NLP']['AIRLINE_ACRONYMS_FILEPATH']\n",
    "AIRLINE_MATCHED_TEXT_PATH = config['NLP']['AIRLINE_MATCHED_TEXT_PATH']\n",
    "AIRLINE_CLEANED_TEXT_PATH = config['NLP']['AIRLINE_CLEANED_TEXT_PATH']\n",
    "GENSIM_DICTIONARY_PATH = config['NLP']['GENSIM_DICTIONARY_PATH']\n",
    "GENSIM_CORPUS_PATH = config['NLP']['GENSIM_CORPUS_PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data on airline fees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>section_name</th>\n",
       "      <th>section_text</th>\n",
       "      <th>criteria</th>\n",
       "      <th>section_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>southwest-airlines-co_annual_report_1994.docx</td>\n",
       "      <td>DEPARTMENT OF TRANSPORTATION RANKINGS FOR 1994...</td>\n",
       "      <td>A multitude of challenges faced the People of ...</td>\n",
       "      <td>&lt;function heading at 0x000001D4AA492EA0&gt;</td>\n",
       "      <td>2849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>southwest-airlines-co_annual_report_1994.docx</td>\n",
       "      <td>RESULTS OF OPERATIONS</td>\n",
       "      <td>1994 COMPARED WITH 1993 The Company's consolid...</td>\n",
       "      <td>&lt;function heading at 0x000001D4AA492EA0&gt;</td>\n",
       "      <td>13806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>southwest-airlines-co_annual_report_1994.docx</td>\n",
       "      <td>ACQUISITION</td>\n",
       "      <td>On December 31, 1993, Southwest exchanged 3,57...</td>\n",
       "      <td>&lt;function heading at 0x000001D4AA492EA0&gt;</td>\n",
       "      <td>2141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>southwest-airlines-co_annual_report_1994.docx</td>\n",
       "      <td>ACCRUED LIABILITIES (IN THOUSANDS) LONG-TERM D...</td>\n",
       "      <td>On March 1, 1993, the Company redeemed the $10...</td>\n",
       "      <td>&lt;function heading at 0x000001D4AA492EA0&gt;</td>\n",
       "      <td>1855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>southwest-airlines-co_annual_report_1995.docx</td>\n",
       "      <td>SECRET NUMBER 1 STICK TO WHAT YOU’RE GOOD AT.</td>\n",
       "      <td>Since 1971, Southwest Airlines has offered sin...</td>\n",
       "      <td>&lt;function style at 0x000001D4AA49F048&gt;</td>\n",
       "      <td>2566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    section_id                                       filename  \\\n",
       "9           10  southwest-airlines-co_annual_report_1994.docx   \n",
       "15          16  southwest-airlines-co_annual_report_1994.docx   \n",
       "23          24  southwest-airlines-co_annual_report_1994.docx   \n",
       "26          27  southwest-airlines-co_annual_report_1994.docx   \n",
       "77          78  southwest-airlines-co_annual_report_1995.docx   \n",
       "\n",
       "                                         section_name  \\\n",
       "9   DEPARTMENT OF TRANSPORTATION RANKINGS FOR 1994...   \n",
       "15                              RESULTS OF OPERATIONS   \n",
       "23                                        ACQUISITION   \n",
       "26  ACCRUED LIABILITIES (IN THOUSANDS) LONG-TERM D...   \n",
       "77      SECRET NUMBER 1 STICK TO WHAT YOU’RE GOOD AT.   \n",
       "\n",
       "                                         section_text  \\\n",
       "9   A multitude of challenges faced the People of ...   \n",
       "15  1994 COMPARED WITH 1993 The Company's consolid...   \n",
       "23  On December 31, 1993, Southwest exchanged 3,57...   \n",
       "26  On March 1, 1993, the Company redeemed the $10...   \n",
       "77  Since 1971, Southwest Airlines has offered sin...   \n",
       "\n",
       "                                    criteria  section_length  \n",
       "9   <function heading at 0x000001D4AA492EA0>            2849  \n",
       "15  <function heading at 0x000001D4AA492EA0>           13806  \n",
       "23  <function heading at 0x000001D4AA492EA0>            2141  \n",
       "26  <function heading at 0x000001D4AA492EA0>            1855  \n",
       "77    <function style at 0x000001D4AA49F048>            2566  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine = create_engine(DB_PATH)\n",
    "df = pd.read_sql(\"SELECT * FROM Sections\", con=engine)\n",
    "\n",
    "# filter to relevant sections\n",
    "df = df[df['section_text'].str.contains('fee')]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A multitude of challenges faced the People of Southwest Airlines in 1994. The mark of a true champion is the ability to “rise to the occasion” and meet challenges. We believe our Employees showed their true Southwest Spirit in 1994, accomplishing three- or four-fold what a normal year would  bring.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store section matches in list\n",
    "text = [section for section in df['section_text'].values]\n",
    "\n",
    "# review first sentence of a section match\n",
    "text[0][0:299]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load spacy nlp model\n",
    "# use 'en' if you don't have the lg model\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Preprocessing - Acronyms\n",
    "\n",
    "SOURCE: https://www.faa.gov/airports/resources/acronyms/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acronym</th>\n",
       "      <th>Definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A/C</td>\n",
       "      <td>Aircraft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A/G</td>\n",
       "      <td>Air to Ground</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A/H</td>\n",
       "      <td>Altitude/Height</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAC</td>\n",
       "      <td>Mike Monroney Aeronautical Center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAF</td>\n",
       "      <td>Army Air Field</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Acronym                          Definition\n",
       "0     A/C                            Aircraft\n",
       "1     A/G                       Air to Ground\n",
       "2     A/H                     Altitude/Height\n",
       "3     AAC   Mike Monroney Aeronautical Center\n",
       "4     AAF                      Army Air Field"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv with airline industry acronyms\n",
    "airline_acronyms = pd.read_csv(AIRLINE_ACRONYMS_FILEPATH)\n",
    "airline_acronyms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise\n",
    "\n",
    "**Curate the acronyms:**\n",
    "\n",
    "1. Convert the acronyms into a dict\n",
    "2. Clean acronyms and definitions (replace spaces with underscores, strip text, lowercase)\n",
    "3. Remove any acronyms that are < two characters (e.g. 'at' == 'air traffic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-d3c3f7b5efaf>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-d3c3f7b5efaf>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    acronym =\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "acronyms = {}\n",
    "\n",
    "for ind, row in airline_acronyms.iterrows():\n",
    "    # get the acronym and convert it to lowercase\n",
    "    acronym = \n",
    "    \n",
    "    # clean acronym definition: \n",
    "    # lower case, strip excess space, replace spaces with underscores to create a single term\n",
    "    definition = \n",
    "    \n",
    "    # add acronyms/definitions pairs to the acronyms dict\n",
    "    # ignore two character acronyms as they often match actual words\n",
    "    # e.g. 'at' == 'air traffic'\n",
    "\n",
    "    \n",
    "# view the first few acronyms\n",
    "list(acronyms.items())[0:5]  # convert to list as dict is not subscriptable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Identify acronyms that exist in text\n",
    "WARNING SLOW!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cat', 'clear')\n",
      "('rnp', 'required_navigation_performance')\n",
      "('mou', 'memorandum_of_understanding')\n",
      "('app', 'approach')\n",
      "('grade', 'graphical_airspace_design_environment')\n",
      "('asm', 'available_seat_mile')\n",
      "('tops', 'telecommunications_ordering_and_pricing_system_(gsa_software_tool)')\n",
      "('dot', 'department_of_transportation')\n",
      "('tsa', 'taxiway_safety_area')\n",
      "('par', 'preferential_arrival_route')\n",
      "('ata', 'air_transport_association_of_america')\n",
      "('aid', 'airport_information_desk')\n",
      "('faa', 'federal_aviation_administration')\n",
      "('self', 'simplified_short_approach_lighting_system_with_sequenced_flashing_lights')\n",
      "('basic', 'basic_contract_observing_station')\n",
      "('gps', 'global_positioning_system')\n",
      "('did', 'direct_inward_dial')\n",
      "('far', 'federal_aviation_regulation')\n",
      "Wall time: 11min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if 1 == 0:\n",
    "    # review the acronyms\n",
    "    acronym_matches = []\n",
    "\n",
    "    # create a nlp pipe to iterate through the text\n",
    "    for doc in nlp.pipe(text, disable=['tagger','ner']):    \n",
    "        # iterate through each word in the sentence\n",
    "        for token in doc:\n",
    "            token = token.text.lower()\n",
    "            # check if token is an acronym\n",
    "            # add matches (acronym and definition) to acronym_matches\n",
    "            if token in acronyms:\n",
    "                acronym_matches.append((token, acronyms[token]))\n",
    "\n",
    "    # review all matching acronyms      \n",
    "    for match in set(acronym_matches):\n",
    "        print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'acronyms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-e17ec44dedb8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0macronyms_to_remove\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'cat'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'app'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'grade'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'self'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'basic'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'did'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'far'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mterm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0macronyms_to_remove\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0macronyms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'acronyms' is not defined"
     ]
    }
   ],
   "source": [
    "# update acronyms list to remove ambiguous acronyms\n",
    "acronyms_to_remove = ['cat','app','grade','self','basic','did','far']\n",
    "for term in acronyms_to_remove:\n",
    "    acronyms.pop(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### collect sentences about fees for phrase model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_phrase_model_sents(matcher, doc, i, matches):\n",
    "    # identify matching spans (phrases)\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "    \n",
    "    # keep only words, lemmatize tokens, remove punctuation\n",
    "    sent = [str(token.lemma_).lower() \n",
    "            for token in span.sent if token.is_alpha]\n",
    "    \n",
    "    # replace acronyms\n",
    "    sent = [acronyms[token] if token in acronyms else token \n",
    "            for token in sent]\n",
    "\n",
    "    # collect matching (cleaned) sents\n",
    "    matched_sents.append(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### match sentences with the word fee or fees\n",
    "\n",
    "WARNING SLOW!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "if 1 == 0:\n",
    "    # match sentences with the word fee or fees\n",
    "    matched_sents = []\n",
    "    pattern = [[{'LOWER': 'fee'}], [{'LOWER': 'fees'}]]\n",
    "\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    \n",
    "    # use *patterns to add more than one pattern at once\n",
    "    matcher.add('fees', collect_phrase_model_sents, *pattern)\n",
    "\n",
    "    for doc in nlp.pipe(text, disable=['tagger','ner']):    \n",
    "        matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of matches: {} \\n'.format(len(matched_sents)))\n",
    "\n",
    "print('Example Match:')\n",
    "print(matched_sents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export matched text to avoid repeating processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment below to write the matched text to a .txt file for later use \n",
    "\n",
    "# with open(AIRLINE_MATCHED_TEXT_PATH, 'w') as f:\n",
    "#    for line in matched_sents:\n",
    "#        line = ' '.join(line) + '\\n'\n",
    "#        line = line.encode('ascii', errors='ignore').decode('ascii') \n",
    "#        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read matched text\n",
    "with open(AIRLINE_MATCHED_TEXT_PATH, 'r') as f:\n",
    "    matched_sents_full = [line for line in f.readlines()]\n",
    "    matched_sents = [line.split() for line in matched_sents_full]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rather than pay the fee demand by this crss we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>these expense include million of various profe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>included in this one time cost result from the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the commitment fee be per annum\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>landing fee and other rental per available_sea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences\n",
       "0  rather than pay the fee demand by this crss we...\n",
       "1  these expense include million of various profe...\n",
       "2  included in this one time cost result from the...\n",
       "3                  the commitment fee be per annum\\n\n",
       "5  landing fee and other rental per available_sea..."
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store all matched senteces in a dataframe\n",
    "matches_df = pd.DataFrame(matched_sents_full, columns=['sentences'])\n",
    "\n",
    "# remove duplicates\n",
    "matches_df = matches_df.drop_duplicates()\n",
    "\n",
    "matches_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SpaCy part of speech (POS) to create phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rather than pay the fee demand by this crss we respond quickly with our own travel agency solution direct access and ticket for the large agency swat overnight delivery of southwest produce ticket for approximately large travel agency improve access to ticket by mail for direct customers by reduce the time limit from seven day out from the date of travel to three day and ticketless travel which eliminate the need to print a paper ticket altogether'"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine the matched sentence tokens and parse it with SpaCy\n",
    "text = ' '.join(matched_sents[0])\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Determine which NLP components can be disabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def view_pos(doc, n_tokens=5):\n",
    "    \"\"\" print SpaCy POS information about each token in a provided document \"\"\"\n",
    "    print('{:15} | {:10} | {:10} | {:30}'.format('TOKEN','POS','DEP_','LEFTS'))\n",
    "    for token in doc[0:n_tokens]:\n",
    "        print('{:15} | {:10} | {:10} | {:30}'.format(\n",
    "            token.text, token.head.pos_,token.dep_, str([t.text for t in token.lefts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN           | POS        | DEP_       | LEFTS                         \n",
      "rather          | ADP        | advmod     | []                            \n",
      "than            | VERB       | advmod     | ['rather']                    \n",
      "pay             | VERB       | ROOT       | ['than']                      \n",
      "the             | NOUN       | det        | []                            \n",
      "fee             | NOUN       | compound   | []                            \n"
     ]
    }
   ],
   "source": [
    "# observe which part of speech (pos) attributes are disabled by named entity recognition (ner)\n",
    "pos_doc = nlp(text, disable=['ner'])\n",
    "view_pos(pos_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN           | POS        | DEP_       | LEFTS                         \n",
      "rather          | ADV        |            | []                            \n",
      "than            | ADP        |            | []                            \n",
      "pay             | VERB       |            | []                            \n",
      "the             | DET        |            | []                            \n",
      "fee             | NOUN       |            | []                            \n"
     ]
    }
   ],
   "source": [
    "# observe which part of speech (pos) attributes are disabled by parser\n",
    "pos_doc = nlp(text, disable=['ner','parser'])\n",
    "view_pos(pos_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN           | POS        | DEP_       | LEFTS                         \n",
      "rather          |            | advmod     | []                            \n",
      "than            |            | advmod     | ['rather']                    \n",
      "pay             |            | ROOT       | ['than']                      \n",
      "the             |            | det        | []                            \n",
      "fee             |            | compound   | []                            \n",
      "demand          |            | dobj       | ['the', 'fee']                \n",
      "by              |            | prep       | []                            \n",
      "this            |            | det        | []                            \n",
      "crss            |            | pobj       | ['this']                      \n",
      "we              |            | nsubj      | []                            \n"
     ]
    }
   ],
   "source": [
    "# observe which part of speech (pos) attributes are disabled by tagger\n",
    "pos_doc = nlp(text, disable=['ner','tagger'])\n",
    "view_pos(pos_doc, n_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'direct object'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use explain to define any token.dep_ attributes\n",
    "spacy.explain('dobj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=https://spacy.io/api/annotation#dependency-parsing width=1000 height=400></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependency_parsing_labels_url = 'https://spacy.io/api/annotation#dependency-parsing'\n",
    "iframe = '<iframe src={} width=1000 height=400></iframe>'.format(dependency_parsing_labels_url)\n",
    "HTML(iframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract phrases by identifying tokens describing an object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add stop words to SpaCy\n",
    "# this enables the .is_stop attribute with common stop words\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "for word in STOP_WORDS:\n",
    "    lex = nlp.vocab[word]\n",
    "    lex.is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seven_day large_swat travel_agency large_agency fee_demand produce_ticket direct_customer time_limit agency_swat agency_solution paper_ticket\n"
     ]
    }
   ],
   "source": [
    "def create_pos_phrases(doc):\n",
    "\n",
    "    doc = nlp(doc, disable=['ner','tagger'])\n",
    "    \n",
    "    phrases = [] \n",
    "    for token in doc:\n",
    "        # find any objects (e.g. direct objects )\n",
    "        if 'obj' in token.dep_:\n",
    "            token_text = token.lemma_.lower()\n",
    "            # find any dependent terms to the left of (preceeding) the object\n",
    "            # ignore dependent terms that are not stop words\n",
    "            for left_term in [t.text for t in token.lefts if t.is_stop == False]:\n",
    "                # combine the dependent term and object, separated by an underscore\n",
    "                # e.g. (travel agency ==> travel_agency)\n",
    "                phrase = '{}_{}'.format(left_term,token_text)\n",
    "                phrases.append(phrase)\n",
    "    \n",
    "    # convert list of distinct phrases into a sentence\n",
    "    return ' '.join(set(phrases))\n",
    "\n",
    "print(create_pos_phrases(matched_sents_full[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# apply the custom function to every element in the dataframe\n",
    "matches_df['pos_phrases'] = matches_df.sentences.apply(create_pos_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>pos_phrases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rather than pay the fee demand by this crss we...</td>\n",
       "      <td>seven_day large_swat travel_agency large_agenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>these expense include million of various profe...</td>\n",
       "      <td>relocation_cost duplicate_property fee_million...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>included in this one time cost result from the...</td>\n",
       "      <td>relocation_cost duplicate_property employee_co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the commitment fee be per annum\\n</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>landing fee and other rental per available_sea...</td>\n",
       "      <td>airport_credit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences  \\\n",
       "0  rather than pay the fee demand by this crss we...   \n",
       "1  these expense include million of various profe...   \n",
       "2  included in this one time cost result from the...   \n",
       "3                  the commitment fee be per annum\\n   \n",
       "5  landing fee and other rental per available_sea...   \n",
       "\n",
       "                                         pos_phrases  \n",
       "0  seven_day large_swat travel_agency large_agenc...  \n",
       "1  relocation_cost duplicate_property fee_million...  \n",
       "2  relocation_cost duplicate_property employee_co...  \n",
       "3                                                     \n",
       "5                                     airport_credit  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pandas Apply\n",
    "\n",
    "apply is an efficient and fast approach to 'apply' a function to every element in a row. applymap does the same to every element in the entire dataframe (e.g. convert all ints to floats)\n",
    "\n",
    "Example: https://chrisalbon.com/python/data_wrangling/pandas_apply_operations_to_dataframes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1  col2\n",
       "0     0     3\n",
       "1     1     4\n",
       "2     2     5"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a small dataframe with example data\n",
    "test_df = pd.DataFrame({'col1':range(0,3),'col2':range(3,6)})\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.0\n",
       "1    1.0\n",
       "2    2.0\n",
       "Name: col1, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply a built-in function to each element in a column\n",
    "test_df['col1'].apply(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5\n",
       "1    6\n",
       "2    7\n",
       "Name: col1, dtype: int64"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply a custom function to every element in a column\n",
    "def add_five(row):\n",
    "    return row + 5\n",
    "\n",
    "test_df['col1'].apply(add_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5\n",
       "1    6\n",
       "2    7\n",
       "Name: col1, dtype: int64"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply an annonomous function to every element in a column\n",
    "test_df['col1'].apply(lambda x: x+5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1  col2\n",
       "0   0.0   3.0\n",
       "1   1.0   4.0\n",
       "2   2.0   5.0"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply a built-in function to every element in a dataframe \n",
    "test_df.applymap(float)  # applymap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocations\n",
    "\n",
    "\"A COLLOCATION is an expression consisting of two or more words that\n",
    "correspond to some conventional way of saying things. Or in the words\n",
    "of Firth (1957: 181): “Collocations of a given word are statements of the\n",
    "habitual or customary places of that word.” Collocations include noun\n",
    "phrases like strong tea and weapons of mass destruction, phrasal verbs like\n",
    "to make up, and other stock phrases like the rich and powerful. Particularly\n",
    "interesting are the subtle and not-easily-explainable patterns of word usage\n",
    "that native speakers all know: why we say a stiff breeze but not a stiff wind\n",
    "(while either a strong breeze or a strong wind is okay), or why we speak of\n",
    "broad daylight (but not bright daylight or narrow darkness)\n",
    "\n",
    "\n",
    "The notion of collocation may be confusing to readers without a background\n",
    "in linguistics. We will devote this section to discussing in more\n",
    "detail what a collocation is.\n",
    "There are actually different definitions of the notion of collocation. Some\n",
    "authors in the computational and statistical literature define a collocation\n",
    "as two or more consecutive words with a special behavior, for example\n",
    "Choueka (1988):\n",
    "[A collocation is defined as] a sequence of two or more consecutive\n",
    "words, that has characteristics of a syntactic and semantic\n",
    "unit, and whose exact and unambiguous meaning or connotation\n",
    "cannot be derived directly from the meaning or connotation of its\n",
    "components.\n",
    "Most of the examples we have presented in this chapter also assumed\n",
    "adjacency of words. But in most linguistically oriented research, a phrase\n",
    "can be a collocation even if it is not consecutive (as in the example knock\n",
    ". . . door). The following criteria are typical of linguistic treatments of collocations\n",
    "(see for example Benson (1989) and Brundage et al. (1992)), noncompositionality\n",
    "being the main one we have relied on here. \n",
    "\n",
    "**Non-compositionality**: The meaning of a collocation is not a straightforward\n",
    "composition of the meanings of its parts. Either the meaning\n",
    "is completely different from the free combination (as in the case of idioms\n",
    "like kick the bucket) or there is a connotation or added element of\n",
    "meaning that cannot be predicted from the parts. For example, white\n",
    "wine, white hair and white woman all refer to slightly different colors, so\n",
    "we can regard them as collocations. \n",
    "\n",
    "**Non-substitutability**: We cannot substitute near-synonyms for the\n",
    "components of a colloction. For example, we can’t say yellow wine\n",
    "instead of white wine even though yellow is as good a description of the\n",
    "color of white wine as white is (it is kind of a yellowish white).\n",
    "\n",
    "**Non-modifiability**: Many collocations cannot be freely modified with\n",
    "additional lexical material or through grammatical transformations.\n",
    "This is especially true for frozen expressions like idioms. For example,\n",
    "we can’t modify frog in to get a frog in one’s throat into to get an ugly\n",
    "frog in one’s throat although usually nouns like frog can be modified by\n",
    "adjectives like ugly. Similarly, going from singular to plural can make\n",
    "an idiom ill-formed, for example in people as poor as church mice.\"\n",
    "\n",
    "SOURCE: https://nlp.stanford.edu/fsnlp/promo/colloc.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Create a function that returns a window of size n over a given sentence. \n",
    "\n",
    "For the sentence **'rather than pay the fee'** return the following if the window is n=3:\n",
    "- ['rather', 'than', 'pay'],\n",
    "- ['than','pay','the']\n",
    "- ['pay', 'the','fee']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rather', 'than', 'pay', 'the', 'fee', 'demand', 'by', 'this', 'crss', 'we', 'respond', 'quickly', 'with', 'our', 'own', 'travel', 'agency', 'solution', 'direct', 'access', 'and', 'ticket', 'for', 'the', 'large', 'agency', 'swat', 'overnight', 'delivery', 'of', 'southwest', 'produce', 'ticket', 'for', 'approximately', 'large', 'travel', 'agency', 'improve', 'access', 'to', 'ticket', 'by', 'mail', 'for', 'direct', 'customers', 'by', 'reduce', 'the', 'time', 'limit', 'from', 'seven', 'day', 'out', 'from', 'the', 'date', 'of', 'travel', 'to', 'three', 'day', 'and', 'ticketless', 'travel', 'which', 'eliminate', 'the', 'need', 'to', 'print', 'a', 'paper', 'ticket', 'altogether']\n"
     ]
    }
   ],
   "source": [
    "# example sentence\n",
    "sent = ' '.join(matches_df['sentences'][0:1]).split()\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['rather', 'than', 'pay'],\n",
       " ['than', 'pay', 'the'],\n",
       " ['pay', 'the', 'fee'],\n",
       " ['the', 'fee', 'demand'],\n",
       " ['fee', 'demand', 'by']]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_sentence_windows(sentence, n=3):\n",
    "    \"create a sliding window over the n terms in a list of terms\"\n",
    "        \n",
    "    # create a window on the first n terms by slicing the sentence into the first n terms\n",
    "    window = \n",
    "    \n",
    "    # create a list to store all windows\n",
    "    # add the first window that was created above\n",
    "    sentence_windows = \n",
    "\n",
    "    # iterate through the rest of the terms of the sentence\n",
    "    # e.g. if n=3, then create a new window with terms 2 to 4\n",
    "    \n",
    "        # remove the first terms of the window and add the next term from the sentence\n",
    "        window = \n",
    "\n",
    "        # add the updated window to the master list\n",
    "        \n",
    "\n",
    "    return sentence_windows\n",
    "\n",
    "# execute the function\n",
    "sentence_window = create_sentence_windows(sent, n=3)\n",
    "# view the first few results\n",
    "sentence_window[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['rather', 'than', 'pay', 'the', 'fee', 'demand', 'by', 'this', 'crss'],\n",
       " ['than', 'pay', 'the', 'fee', 'demand', 'by', 'this', 'crss', 'we'],\n",
       " ['pay', 'the', 'fee', 'demand', 'by', 'this', 'crss', 'we', 'respond'],\n",
       " ['the', 'fee', 'demand', 'by', 'this', 'crss', 'we', 'respond', 'quickly'],\n",
       " ['fee', 'demand', 'by', 'this', 'crss', 'we', 'respond', 'quickly', 'with']]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute the function for all sentences\n",
    "\n",
    "# create a list to store all windows\n",
    "sentence_window = []\n",
    "\n",
    "for sent in matches_df['sentences']:\n",
    "    # convert the sentence string into a list of terms\n",
    "    sent = sent.split()\n",
    "    \n",
    "    # create the sentence windows and append to the sentence_windows list\n",
    "    windows = create_sentence_windows(sent, n=3)\n",
    "    \n",
    "    # add each window to the sentence_window list\n",
    "    # iterate through windows to make each item in sentence window a window, not a list of windows\n",
    "    for window in windows:\n",
    "        sentence_window.append(window)\n",
    "\n",
    "# view the first five results\n",
    "sentence_window[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('security_fee', 206),\n",
       " ('fee_rental', 172),\n",
       " ('increase_fee', 171),\n",
       " ('check_bag', 151),\n",
       " ('landing_fee', 148),\n",
       " ('increase_percent', 125),\n",
       " ('rental_expense', 123),\n",
       " ('fee_passenger', 123),\n",
       " ('impose_fee', 120),\n",
       " ('landing_rental', 114),\n",
       " ('baggage_fee', 114),\n",
       " ('bag_fee', 113),\n",
       " ('additional_fee', 113),\n",
       " ('change_law', 105),\n",
       " ('1_quarter', 104),\n",
       " ('fee_pay', 98),\n",
       " ('1_bag', 96),\n",
       " ('rental_increase', 95),\n",
       " ('seat_selection', 94),\n",
       " ('fee_expense', 94)]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "# create a defaultdict to keep track of common phrases\n",
    "window_count = defaultdict(int)\n",
    "\n",
    "for sent in sentence_window:\n",
    "    # remove stop words\n",
    "    sentence = [term for term in sent if term not in STOP_WORDS]\n",
    "    \n",
    "    # create a combination of terms\n",
    "    # e.g. (rather, than, pay) --> (rather,than), (than,pay), (rather,pay)\n",
    "    for combo in combinations(sentence, 2):\n",
    "        # convert the tuple to a term\n",
    "        # e.g. (rather, than) --> 'rather_than'\n",
    "        phrase = '_'.join(combo)\n",
    "\n",
    "        # increment the count for the term each time it appears to identify the most common terms\n",
    "        window_count[phrase] += 1\n",
    "\n",
    "# sort to view the most common terms\n",
    "# the key (lambda x: x[1]) sorts by the count\n",
    "sorted(window_count.items(), key=lambda x: x[1], reverse=True)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase (collocation) Detection\n",
    "\n",
    "Phrase modeling is another approach to learning combinations of tokens that together represent meaningful multi-word concepts. We can develop phrase models by looping over the the words in our reviews and looking for words that co-occur (i.e., appear one after another) together much more frequently than you would expect them to by random chance. The formula our phrase models will use to determine whether two tokens $A$ and $B$ constitute a phrase is:\n",
    "\n",
    "$$\\frac{count(A\\ B) - count_{min}}{count(A) * count(B)} > threshold$$\n",
    "\n",
    "- $count(A)$ is the number of times token $A$ appears in the corpus\n",
    "- $count(B)$ is the number of times token $B$ appears in the corpus\n",
    "- $count(A\\ B)$ is the number of times the tokens $A\\ B$ appear in the corpus in order\n",
    "- $count_{min}$ is a user-defined parameter to ensure that accepted phrases occur a minimum number of times\n",
    "- $threshold$ is a user-defined parameter to control how strong of a relationship between two tokens the model requires before accepting them as a phrase\n",
    "\n",
    "Once our phrase model has been trained on our corpus, we can apply it to new text. When our model encounters two tokens in new text that identifies as a phrase, it will merge the two into a single new token.\n",
    "\n",
    "Phrase modeling is superficially similar to named entity detection in that you would expect named entities to become phrases in the model (so new york would become new_york). But you would also expect multi-word expressions that represent common concepts, but aren't specifically named entities (such as happy hour) to also become phrases in the model.\n",
    "\n",
    "We turn to the indispensible gensim library to help us with phrase modeling — the Phrases class in particular.\n",
    "\n",
    "SOURCE: \n",
    "- https://github.com/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb\n",
    "- https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scikit-learn API for Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alsherman\\AppData\\Local\\Continuum\\anaconda3\\envs\\guild\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PhrasesTransformer(delimiter=b'_', max_vocab_size=40000000, min_count=3,\n",
       "          progress_per=10000, scoring='default', threshold=3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.sklearn_api.phrases import PhrasesTransformer\n",
    "\n",
    "sklearn_phrases = PhrasesTransformer(min_count=3, threshold=3)\n",
    "sklearn_phrases.fit(matched_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alsherman\\AppData\\Local\\Continuum\\anaconda3\\envs\\guild\\lib\\site-packages\\gensim\\models\\phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the_taxiway_safety_area', 'available_seat_mile_increase', 'federal_aviation_administration', 'the_department_of_transportation', 'per_available_seat_mile', 'available_seat_mile', 'required_navigation_performance'}\n"
     ]
    }
   ],
   "source": [
    "# review phrase matches\n",
    "phrases = []\n",
    "for terms in sklearn_phrases.transform(matched_sents):\n",
    "    for term in terms:\n",
    "        if term.count('_') >= 2:\n",
    "            phrases.append(term)\n",
    "print(set(phrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a list of stop words\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "common_terms = list(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**common_terms:** optional list of “stop words” that won’t affect frequency count of expressions containing them.\n",
    "    - The common_terms parameter add a way to give special treatment to common terms (aka stop words) such that their presence between two words won’t prevent bigram detection. It allows to detect expressions like “bank of america” or “eye of the beholder”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gensim API\n",
    "A more complex API, though it is faster and has better integration with other gensim components (e.g. Phraser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases\n",
    "from gensim.models.phrases import Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.phrases.Phrases at 0x29a8bd9f0f0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases = Phrases(\n",
    "      matched_sents\n",
    "    , common_terms=common_terms\n",
    "    , min_count=3\n",
    "    , threshold=3\n",
    "    , scoring='default'\n",
    ")\n",
    "\n",
    "phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrases Params\n",
    "\n",
    "- **scoring:** specifies how potential phrases are scored for comparison to the threshold setting. scoring can be set with either a string that refers to a built-in scoring function, or with a function with the expected parameter names. Two built-in scoring functions are available by setting scoring to a string:\n",
    "\n",
    "    - ‘default’: from “Efficient Estimaton of Word Representations in Vector Space” by Mikolov, et. al.: \n",
    "    \n",
    "$$\\frac{count(AB) - count_{min}}{count(A) * count(B)} * N > threshold$$\n",
    "    \n",
    "\n",
    "    - where N is the total vocabulary size.\n",
    "    - Thus, it is easier to exceed the threshold when the two words occur together often or when the two words are rare (i.e. small product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.phrases.Phraser at 0x29a8be36ef0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram = Phraser(phrases)\n",
    "\n",
    "bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The phrases object still contains all the source text in memory. A gensim Phraser will remove this extra data to become smaller and somewhat faster than using the full Phrases model. To determine what data to remove, the Phraser ues the  results of the source model’s min_count, threshold, and scoring settings. (You can tamper with those & create a new Phraser to try other values.)\n",
    "\n",
    "SOURCE: https://radimrehurek.com/gensim/models/phrases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_phrases(phraser, text_stream, num_underscores=2):\n",
    "    \"\"\" identify phrases from a text stream by searching for terms that\n",
    "    are separated by underscores and include at least num_underscores\n",
    "    \"\"\"\n",
    "    \n",
    "    phrases = []\n",
    "    for terms in phraser[text_stream]:\n",
    "        for term in terms:\n",
    "            if term.count('_') >= num_underscores:\n",
    "                phrases.append(term)\n",
    "    print(set(phrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fee_for_optional', 'require_among_other_thing', 'advance_of_travel', 'day_in_advance', 'attempt_to_monopolize', 'remit_this_back_to_the_applicable', 'service_be_provide', 'conjurer_the_only_major', 'advantage_of_such_service', 'hour_without_make_a_payment', 'limit_or_regulate', 'complete_and_ongoing', 'cost_for_safety', 'pay_for_ancillary', 'delay_of_much_than_minute', 'follow_the_acquisition', 'service_on_their_website', 'measure_have_conjurer', 'taxiway_safety_area', 'disclose_all_potential', 'duplicate_or_incompatible', 'cancel_a_pay', 'difference_in_airfare', 'new_and_expand', 'facility_at_each_of_the_airport', 'service_such_a_the_transportation', 'stand_the_claim', 'component_of_the_passenger', 'legislation_which_can_result', 'gain_a_competitive', 'curb_side_checkin', 'reservation_be_make_a_long', 'change_to_environmental', 'pay_to_airtran', 'business_through_the_imposition', 'primarily_a_a_result', 'pay_a_difference', 'limitation_on_route', 'rate_and_charge', 'promote_the_company', 'cost_be_then_allocate', 'enter_into_fuel', 'use_to_support', 'notify_in_the_event', 'primarily_due_to_high', 'taxiway_safety_area_to_assess', 'passenger_be_allow', 'finance_may_conjurer', 'doe_not_impose', 'reservation_without_penalty', 'airtran_and_delta', 'department_of_transportation_propose', 'propose_to_congress', 'penalty_for_hour', 'permit_and_approval', 'damage_on_behalf', 'imposition_of_a_1', 'payments_be_expect', 'reduce_their_capacity', 'range_of_allege', 'congress_may_consider', 'difficulty_in_obtain', 'airtran_to_have_a_consistent', 'continue_to_be_the_only_major', 'fee_must_apply', 'percent_of_operating', 'generate_much_net', 'impose_a_annual', 'maximum_of_per_one_way', 'hour_after_the_reservation', 'deduction_and_preference', 'result_in_a_low', 'damage_for_the_amount_of_1', 'minimum_of_per_one_way', 'fee_must_be_disclose', 'long_a_the_reservation', 'service_if_a_flight', 'rule_by_implement', 'confirmation_and_viii', 'capacity_or_use', 'federal_aviation_administration', 'reimbursement_of_the_company', 'allow_to_hold', 'fee_for_permanently', 'cost_the_taxiway_safety_area', 'taxis_and_fee', 'subject_to_a_maximum', 'affect_through_future', 'information_for_basic', 'purchase_of_miles', 'numb_of_total', 'domestic_and_foreign', 'conspiracy_with_respect', 'action_and_decision', 'allow_to_cancel', 'fare_be_refundable', 'project_the_majority', 'reservation_be_make_at_little', 'relief_against_a_broad', 'law_that_affect', 'plan_can_change', 'hold_a_reservation', 'respectfully_with_its_low', 'available_seat_mile_basis', 'rate_and_the_elimination', 'operation_can_be_negative', 'website_and_iv', 'passenger_be_unable', 'million_or_percent', 'doe_not_charge', 'item_such_a_seat', 'available_seat_mile', 'generate_much_government', 'include_but_not_limit', 'changeable_and_include', 'conspire_with_delta', 'include_in_aircraft', 'pay_a_usage', 'thing_that_airtran', 'operation_of_customs', 'person_or_entity', 'unlike_much_of_its_competitor', 'charge_a_change', 'intend_upon_full_integration', 'impact_the_company', 'department_of_transportation', 'cancel_or_oversell', 'increase_after_purchase', 'grants_to_airport', 'access_at_slot', 'consist_of_distribution', 'lease_and_for_terminal', 'effort_to_reduce', 'rental_per_available_seat_mile', 'passenger_at_the_time', 'confirmation_and_vii', 'delta_the_consolidated', 'fee_and_other_rental', '1_and_2', 'customers_on_behalf', 'change_in_law', 'primarily_due_to_consult', 'restriction_on_competitive', 'cancellation_or_diversion', '1_or_2', 'jurisdiction_over_its_operation', 'asif_on_each_airline', 'cost_such_a_credit', 'additionally_when_other_airline', 'airfare_the_customer', 'fund_for_passenger', 'extension_or_transfer', 'importance_to_southwest', 'required_navigation_performance', 'fare_may_not_increase', 'department_of_transportation_far', 'decision_that_create', 'passenger_must_be_promptly', 'initiative_a_previously', 'item_such_a_1', 'payment_of_facilities', 'hedge_against_increase', 'fare_and_no_unexpected', 'report_much_information', 'capacity_and_price', 'offer_by_airline', 'addition_to_treble', 'require_that_i_advertise', 'allocate_among_a_few_numb', 'advantage_by_differentiate', 'refund_any_check', 'violation_of_section', 'security_be_provide', 'share_with_ticket', 'reservation_for_up_to_hour', 'typically_the_flight', 'necessary_to_cover', 'pay_up_to_in_deny', 'bump_from_flight', 'august_the_court', 'grant_to_the_taxiway_safety_area', 'serve_to_which_various_leasehold', 'fare_and_the_adoption', 'pursuant_to_authority', 'department_of_transportation_on_the_amount_and_type', 'change_and_therefore_doe', 'airline_that_doe'}\n"
     ]
    }
   ],
   "source": [
    "print_phrases(bigram, matched_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tri-gram phrase model\n",
    "\n",
    "We can place the text from the first phrase model into another Phrases object to create n-term phrase models. We can repear this process multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'violate_section_of_the_sherman_act', 'landing_fee_and_other_rental_respectively', 'amended_complaint_seek_injunctive', 'passenger_protection_rules_conjurer', '1_item_of_check_luggage', 'flight_change_seat', 'limit_or_regulate', 'advantage_of_such_service', 'cost_for_safety', 'company_currently_expect_landing', 'cost_the_taxiway_safety_area_have_impose_a_annual', 'lose_luggage_iii_prominently', 'follow_the_acquisition', 'conspire_with_delta_in_impose_bag', 'measure_have_conjurer', 'generate_much_net_federal_revenue', 'available_seat_mile_basis_landing_fee', 'taxiway_safety_area', 'attempt_to_monopolize_air_travel', 'duplicate_or_incompatible', 'airtran_and_delta_have_violate_section', 'customer_change_in_flight', 'difference_in_airfare', 'ancillary_service_on_their_website_and_iv', 'facility_at_each_of_the_airport', 'reservation_be_make_at_little_seven_day', 'fee_for_permanently_lose_luggage', 'fare_and_the_adoption', 'initial_complaint_seek_treble', 'legislation_which_can_result', 'gain_a_competitive', 'expense_be_include_in_aircraft', 'require_that_i_advertise_airfares_include', 'company_be_conjurer_require', 'change_to_environmental', 'pay_to_airtran_and_to_delta_the_consolidated', 'business_through_the_imposition', 'airfare_the_customer_will_not_be_charge_a_change', 'treat_customers_fairly_honestly', 'primarily_a_a_result', 'limitation_on_route', 'landing_fee_and_other_rental_per_available_seat_mile', 'domestic_flight_begin_december', 'conspiracy_with_respect_to_the_imposition_of_a_1', 'new_and_expand_component_of_the_passenger', 'rate_and_charge', 'approve_legislation_in_december', 'imposition_of_a_1_bag', 'promote_the_company', 'airtran_currently_charge', 'long_a_the_reservation_be_make_at_little_seven', 'respectfully_with_its_low_fare_and_no_unexpected', 'enter_into_fuel', 'capacity_and_price_decision', 'use_to_support', 'primarily_due_to_high', 'air_carrier_if_necessary_to_cover', 'taxiway_safety_area_to_assess', 'finance_may_conjurer', 'include_in_aircraft_rental', 'increase_by_million_or_percent', 'doe_not_impose', 'relief_against_a_broad_range_of_allege', 'department_of_transportation_propose', 'bump_from_flight_ii_refund', 'propose_to_congress', 'permit_and_approval', 'security_be_provide_in_part_by_a_per_enplanement_security', 'confirmation_and_vii_passenger_must_be_promptly', 'campaign_emphasize_southwest_approach', 'payments_be_expect', 'service_such_a_the_transportation_of_unaccompanied_minor', 'reduce_their_capacity', 'ii_passenger_be_allow_to_hold', 'congress_may_consider', 'difficulty_in_obtain', 'pet_liquor_sale_advance', 'airtran_to_have_a_consistent', 'continue_to_be_the_only_major', 'percent_of_operating', 'bag_fee_for_permanently', 'maximum_of_per_one_way', 'deduction_and_preference', 'subject_to_a_maximum_of_per_one_way_trip', 'pursuant_to_authority_grant_to_the_taxiway_safety_area', 'result_in_a_low', 'check_bag_fee_for_permanently', 'seat_assignment_call_center_service', 'aircraft_operate_lease_and_for_terminal', 'violation_of_section_of_the_sherman_act', 'cancel_or_oversell_and_a_passenger_be_unable', 'stand_the_claim_of_a_conspiracy_with_respect', 'minimum_of_per_one_way', 'reservation_for_up_to_hour_without_make_a_payment_iii', 'confirmation_and_viii_passenger_must_be_promptly', 'passenger_at_the_time_of_book_v', 'rule_by_implement', 'damage_on_behalf_of_a_putative_class', 'priority_seat_selection_special', 'refund_any_check_bag', 'passenger_from_per_passenger_segment', 'disclose_all_potential_fee_for_optional', 'southwest_be_conjurer_the_only_major', 'cost_be_then_allocate_among_a_few_numb_of_total', 'passenger_facility_charge', 'federal_aviation_administration', 'reimbursement_of_the_company', 'capacity_or_use', 'atsa_fund_for_passenger', 'federal_transportation_taxis_federal', 'approximate_million_for_southwest', 'pay_for_ancillary_service_if_a_flight', 'taxis_and_fee', 'service_be_provide_which_be_typically_the_flight', 'cancellation_or_diversion_of_their_flight', 'affect_through_future', 'information_for_basic', 'rental_per_available_seat_mile_increase_percent', 'generally_recognize_a_other_revenue', 'domestic_and_foreign', 'operation_lease_expense', 'action_and_decision', 'addition_to_treble_damage_for_the_amount_of_1', 'fare_may_not_increase_after_purchase_v', 'fare_be_refundable', 'allow_to_cancel_a_pay_reservation', 'project_the_majority', 'effort_to_reduce_the_federal_deficit', 'law_that_affect', 'passenger_at_the_time_of_book_vi', 'purchase_of_miles_rewards', 'lease_and_for_terminal_operation_lease', 'generate_much_government_revenue_congress', 'additional_federal_aviation_security', 'available_seat_mile_basis', 'rate_and_the_elimination', 'campaign_highlight_the_importance_to_southwest', 'operation_can_be_negative', 'customer_service_by_show_that_southwest_understand', 'website_and_iv', 'customer_may_pay_a_difference', 'rental_and_in_landing_fee', 'penalty_for_hour_after_the_reservation_be_make_a_long', '1_or_2_bag', 'item_such_a_seat', 'available_seat_mile', 'baggage_fee_must_be_disclose', 'include_but_not_limit', 'check_baggage_carriage', 'changeable_and_include', 'complaint_allege_among_other_thing_that_airtran', 'cost_associate_with_complete_and_ongoing', 'pay_a_usage', 'court_let_stand_the_claim', 'operation_of_customs', 'remit_this_back_to_the_applicable_governmental_entity', 'unlike_much_of_its_competitor', 'curb_side_checkin_and_telephone_reservation', 'charge_a_change', 'intend_upon_full_integration', 'impact_the_company', 'department_of_transportation', 'protection_rule_require_among_other_thing', 'grants_to_airport', 'access_at_slot', 'consist_of_distribution', 'anticompetitive_activity_a_good', 'aviation_security_infrastructure_fee', 'fee_must_be_disclose_on_e_ticket', 'item_such_a_1_and_2_check', 'rental_per_available_seat_mile', 'day_in_advance_of_travel_iv', 'advance_of_travel_iii_fare', 'fee_and_other_rental', '1_and_2', 'customers_on_behalf', 'way_passenger_trip', 'change_in_law', 'primarily_due_to_consult', 'restriction_on_competitive', '1_or_2', 'baggage_allowance_and_fee_must_apply', 'reservation_without_penalty_for_hour_after_the_reservation', 'plan_can_change_and_therefore_doe_not_charge', 'directly_pay_delta_airtran', 'jurisdiction_over_its_operation', 'extension_or_transfer_of_miles_rewards', 'asif_on_each_airline', 'cost_such_a_credit', 'additionally_when_other_airline', 'fund_for_passenger', 'amounts_collect_from_passenger', 'pay_by_airline_directly', 'flight_ii_refund_any_check', 'required_navigation_performance', 'increase_after_purchase_iv', 'department_of_transportation_far', 'decision_that_create', 'notify_in_the_event_of_delay_of_much_than_minute', 'conjurer_the_only_major_airline_that_doe', 'rental_respectively_in_the_consolidated_statement', 'payment_of_facilities', 'passenger_be_allow_to_hold_a_reservation', 'initiative_a_previously', 'landing_fee_and_other_rental_expense', 'report_much_information', 'service_on_their_website_and_iv_refund', 'offer_by_airline', 'hedge_against_increase', 'allocate_among_a_few_numb', 'collect_certain_taxis_and_fee', 'advantage_by_differentiate', 'passenger_trip_vi_baggage', 'department_of_transportation_on_the_amount_and_type', 'august_the_court_dismiss_claim', 'passenger_be_allow_to_cancel_a_pay', 'effective_july_and_ii_eliminate', 'share_with_ticket', 'require_airline_to_i_pay_up_to_in_deny', 'consolidated_statement_of_income', 'unlike_much_of_its_competitor_southwest_doe', 'customers_on_behalf_of_government_agency', 'hour_without_make_a_payment_ii_passenger', 'august_the_court', 'result_in_increase_land', 'serve_to_which_various_leasehold', 'passenger_trip_vii_baggage', 'selection_fuel_surcharge_snack', 'person_or_entity_in_the_united_states', 'change_and_therefore_doe', 'airline_that_doe', 'sherman_act_the_court_let'}\n"
     ]
    }
   ],
   "source": [
    "phrases = Phrases(bigram[matched_sents], common_terms=common_terms, min_count=5,threshold=5)\n",
    "trigram = Phraser(phrases)\n",
    "\n",
    "print_phrases(trigram, bigram[matched_sents], num_underscores=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOC NUMBER: 5\n",
      "\n",
      "ORIGINAL SENTENT: landing fee and other rental per available_seat_mile increase percent in compare to which include a airport credit of million\n",
      "\n",
      "BIGRAM: landing_fee and other rental_per_available_seat_mile increase_percent in compare to which include a airport credit of million\n",
      "\n",
      "TRIGRAM: landing_fee_and_other_rental_per_available_seat_mile increase_percent in compare to which include a airport credit of million\n"
     ]
    }
   ],
   "source": [
    "for doc_num in [5]:\n",
    "    print('DOC NUMBER: {}\\n'.format(doc_num))\n",
    "    print('ORIGINAL SENTENT: {}\\n'.format(' '.join(matched_sents[doc_num])))\n",
    "    print('BIGRAM: {}\\n'.format(' '.join(bigram[matched_sents[doc_num]])))\n",
    "    print('TRIGRAM: {}'.format(' '.join(trigram[bigram[matched_sents[doc_num]]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export Cleaned Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write the cleaned text to a new file for later use\n",
    "with open(AIRLINE_CLEANED_TEXT_PATH, 'w') as f:\n",
    "    for line in bigram[matched_sents]:\n",
    "        line = ' '.join(line) + '\\n'\n",
    "        line = line.encode('ascii', errors='ignore').decode('ascii')\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced - clean text using SpaCy and gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7b534a76b06f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m# combined terms after phrase model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mafter_phrase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mterm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'doc' is not defined"
     ]
    }
   ],
   "source": [
    "def clean_text(doc):\n",
    "    print(doc, '\\n')\n",
    "\n",
    "    ents = nlp(doc.text).ents\n",
    "\n",
    "    # Add named entities, but only if they are a compound of more than word.\n",
    "    IGNORE_ENTS = ('QUANTITY','ORDINAL','CARDINAL','DATE'\n",
    "                   ,'PERCENT','MONEY','TIME')\n",
    "    ents = [ent for ent in ents if \n",
    "             (ent.label_ not in IGNORE_ENTS) and (len(ent) > 2)]\n",
    "    \n",
    "    # add underscores to combine words in entities\n",
    "    ents = [str(ent).strip().replace(' ','_') for ent in ents]\n",
    "    \n",
    "    # clean text for phrase model\n",
    "    # Keep only words (no numbers, no punctuation).\n",
    "    # Lemmatize tokens, remove punctuation and remove stopwords.\n",
    "    doc_ = [token.lemma_ for token in doc if token.is_alpha]\n",
    "    phrase_text = [str(term) for term in doc_]\n",
    "    sent = bigram[phrase_text]\n",
    "    phrases = []\n",
    "    for term in sent:\n",
    "        if '_' in term:\n",
    "            phrases.append(term)\n",
    "\n",
    "    # remove stops words - \n",
    "    # separate step as they are needed for the phrase model\n",
    "    doc = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "    # add phrases and entities\n",
    "    doc.extend([entity for entity in ents])\n",
    "    clean_text = [str(term) for term in doc] + phrases\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "# combined terms after phrase model\n",
    "after_phrase = []\n",
    "for sent in doc.sents:\n",
    "    text = clean_text(sent)\n",
    "    for term in text:\n",
    "        if '_' in term:\n",
    "            after_phrase.append(term)\n",
    "\n",
    "print(set(after_phrase))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
